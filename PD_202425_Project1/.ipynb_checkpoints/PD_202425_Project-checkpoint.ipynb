{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining / Prospecção de Dados\n",
    "\n",
    "## Sara C. Madeira, 2024/2025\n",
    "\n",
    "# Project 1 - Pattern Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Logistics \n",
    "**_Read Carefully_**\n",
    "\n",
    "**Students should work in teams of 3 people**. \n",
    "\n",
    "Groups with less than 3 people might be allowed (with valid justification), but will not have better grades for this reason. \n",
    "\n",
    "The quality of the project will dictate its grade, not the number of people working.\n",
    "\n",
    "**The project's solution should be uploaded in Moodle before the end of `May, 4th (23:59)`.** \n",
    "\n",
    "Students should **upload a `.zip` file** containing a folder with all the files necessary for project evaluation. \n",
    "Groups should be registered in [Moodle](https://moodle.ciencias.ulisboa.pt/mod/groupselect/view.php?id=139096) and the `zip` file should be identified as `PDnn.zip` where `nn` is the number of your group.\n",
    "\n",
    "**It is mandatory to produce a Jupyter notebook containing code and text/images/tables/etc describing the solution and the results. Projects not delivered in this format will not be graded. You can use `PD_202425_P1.ipynb` as template. In your `.zip` folder you should also include an HTML version of your notebook with all the outputs.**\n",
    "\n",
    "**Decisions should be justified and results should be critically discussed.** \n",
    "\n",
    "Remember that **your notebook should be as clear and organized as possible**, that is, **only the relevant code and experiments should be presented, not everything you tried and did not work, or is not relevant** (that can be discussed in the text, if relevant)! Tables and figures can be used together with text to summarize results and conclusions, improving understanding, readability and concision. **More does not mean better! The target is quality not quantity!**\n",
    "\n",
    "_**Project solutions containing only code and outputs without discussions will achieve a maximum grade of 10 out of 20.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Tools\n",
    "\n",
    "The dataset to be analysed is **`Foodmart_2025_DM.csv`**, which is a modified and integrated version of the **Foodmart database**, used in several [Kaggle](https://www.kaggle.com) Pattern Mining competitions, with the goal of finding **actionable patterns** by analysing data from the `FOODmart Ltd` company, a leading supermarket chain. \n",
    "\n",
    "`FOODmart Ltd` has different types of stores: Deluxe Supermarkets, Gourmet Supermarkets, Mid-Size Grocerys, Small Grocerys and \n",
    "Supermarkets.\n",
    "\n",
    "Your **goals** are to find: \n",
    "1. **global patterns** (common to all stores) and\n",
    "2. **local/specific patterns** (related to the type of store).\n",
    "\n",
    "**`Foodmart_2025_DM.csv`** stores **69549 transactions** from **24 stores**, where **103 different products** can be bought. \n",
    "\n",
    "Each transaction (row) has a `STORE_ID` (integer from 1 to 24), and a list of produts (items), together with the quantities bought. \n",
    "\n",
    "In the transation highlighted below, a given customer bought 1 unit of soup, 2 of cheese and 1 of wine at store 2.\n",
    "\n",
    "<img src=\"Foodmart_2025_DM_Example.png\" alt=\"Foodmart_2025_DM_Example\" style=\"width: 1000px;\"/>\n",
    "\n",
    "In this context, the project has **2 main tasks**:\n",
    "1. Mining Frequent Itemsets and Association Rules: Ignoring Product Quantities and Stores **(global patterns)**\n",
    "2. Mining Frequent Itemsets and Association Rules: Looking for Differences between Stores **(local/specific patterns)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **While doing PATTERN and ASSOCIATION MINING keep in mind the following basic/key questions and BE CREATIVE!**\n",
    "\n",
    "### 1. What are the most popular products?\n",
    "### 2. Which products are bought together?\n",
    "### 3. What are the frequent patterns?\n",
    "### 4. Can we find associations highlighting that when people buy a product/set of products also buy other product(s)?\n",
    "### 5. Are these associations strong? Can we trust them? Are they misleading?\n",
    "### 6. Can we analyse these patterns and evaluate these associations to find, not only frequent and strong associations, but also interest patterns and associations?\n",
    "\n",
    "**In this project you should use [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org) and [`MLxtend`](http://rasbt.github.io/mlxtend/).**\n",
    "\n",
    "When using `MLxtend`, frequent patterns can either be discovered using `Apriori` and `FP-Growth`. **Choose the pattern mining algorithm to be used.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP-growth is more efficient with larger datasets than apriori; however it can be memory-intensive, especially for datasets with many frequent items. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Identification\n",
    "\n",
    "**GROUP 09**\n",
    "\n",
    "Students:\n",
    "\n",
    "* Daniel João - *56455*\n",
    "* Daniel Ribeiro - *64476*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mining Frequent Itemsets and Association Rules: Ignoring Product Quantities and Stores\n",
    "\n",
    "In this first task you should load and preprocessed the dataset **`Foodmart_2025_DM.csv`** in order to compute frequent itemsets and generate association rules considering all the transactions, regardeless of the store, and ignoring product quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might need to install an older version of *mlxtend* cuz of the issue we saw on the TP classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlxtend==0.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend that all use a conda environment to keep these packages separate:\n",
    "(in terminal/ bash)\n",
    "* 1. Create a conda env: **\"conda create -n env_name python=3.9\"**\n",
    "* 2. Activate your conda environment: **\"conda activate env_name\"**\n",
    "* 3. Install the correct package version: **\"conda install -c conda-forge mlxtend=0.19\"**  (THIS IS IMPORTANT)\n",
    "* 4. Install other packages like pandas and such ...\n",
    "* 5. Install kernel package so you can use this env has a kernel on Jupyter Notebook: **\"conda install -c conda-forge ipykernel\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. Used Packages Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib as mpl\n",
    "import plotly.express as px         # it's nice to make plots interactable on the notebook and HTML\n",
    "import plotly.io as pio\n",
    "import networkx as nx\n",
    "pio.renderers.default = 'notebook_connected'\n",
    "\n",
    "\n",
    "# mining patterns\n",
    "# BEWARE mlxtend version 0.19 \n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules, fpmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set pandas to display all columns of a df:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display columns' content fully\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load and Preprocess Dataset\n",
    "\n",
    "**Product quantities and stores should not be considered.**\n",
    "\n",
    "- Removing STORE_ID info; each row is a transaction.\n",
    "- Values transformation, from Integer to Boolean (One-Hot Encoding):\n",
    "    - **True** if value > 1,\n",
    "    - **False** if value = 0.\n",
    "- each row is a transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. Fixing Malformed Lines \n",
    "\n",
    "<ins>Fifty-four lines in the CSV are malformed</ins> (confirmation is possible right below), having **missing commas** between product types like the following in cell[7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Foodmart_2025_DM.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(?<=[0-9])(?=[A-Za-z])')\n",
    "\n",
    "# Read the file lines\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Find lines that match the pattern\n",
    "bad_lines = [line.strip() for line in lines if pattern.search(line)]\n",
    "\n",
    "# Print the first 10 problematic lines\n",
    "print(\"Lines with missing commas (showing first 10):\")\n",
    "for line in bad_lines[:10]:\n",
    "    print(line)\n",
    "\n",
    "print(len(bad_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_items(lines):\n",
    "    \"\"\"\n",
    "    Given a list of transaction lines (each like \"STORE_ID=2,ItemA=3,ItemB=1,…\"),\n",
    "    return the set of all product names.\n",
    "\n",
    "    Args:\n",
    "      lines (List[str]): raw CSV lines, one transaction per element.\n",
    "\n",
    "    Returns:\n",
    "      Set[str]: every distinct key to the left of the “=” (excluding STORE_ID).\n",
    "    \"\"\"\n",
    "    items = set()\n",
    "    for line in lines:\n",
    "        # split off the STORE_ID field\n",
    "        parts = line.split(',')[1:]\n",
    "        for p in parts:\n",
    "            if '=' in p:\n",
    "                # rsplit on the last '=' so we don’t chop product names\n",
    "                product, qty = p.rsplit('=', 1)\n",
    "                items.add(product.strip())\n",
    "    return items\n",
    "\n",
    "raw_items = extract_items(open(filename).read().splitlines())\n",
    "cleaned_items = extract_items(re.sub(r'(?<=[0-9])(?=[A-Za-z])', ',', open(filename).read()).splitlines())\n",
    "print(\"Raw count:\", len(raw_items))        # 136\n",
    "print(\"Cleaned count:\", len(cleaned_items))# 102\n",
    "print(\"Dropped composites:\", sorted(raw_items - cleaned_items))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing that (no number should be immediately before a letter, since the malformed lines only have letters after the number) and opening content in the panda's data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transaction_csv(filename):\n",
    "    \"\"\"\n",
    "    Reads the CSV file containing transactions, fixes missing commas between key-value pairs,\n",
    "    processes the transactions, and returns a list of transactions.\n",
    "    Requires:\n",
    "        - The CSV file at `filename` is accessible and readable.\n",
    "        - Each line in the file is formatted with key-value pairs in the format \"Key=Value\".\n",
    "        - Some rows may be missing commas between consecutive key-value pairs\n",
    "          (e.g., \"Pasta=2Waffles=2\" should be \"Pasta=2,Waffles=2\").\n",
    "        - The value for each key is numeric, and only keys with a numeric value above 0 should be considered.\n",
    "        - Keys that start with a specific prefix (e.g., \"STORE_ID\") are considered auxiliary and will be ignored.\n",
    "    Ensures: The function returns a list of transactions, where each transaction is a list of keys (products) \n",
    "    that had a numeric value greater than 0.\n",
    "    Returns: list: A list of transactions, each transaction is a list of product keys.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    data_fixed = re.sub(r'(?<=[0-9])(?=[A-Za-z])', ',', data)\n",
    "    \n",
    "    # Create a file-like object from the fixed data.\n",
    "    csvfile = io.StringIO(data_fixed)\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    \n",
    "    dataset = []\n",
    "    for row in reader:\n",
    "        transaction = []\n",
    "        for kv in row:\n",
    "            if '=' in kv:\n",
    "                key, value = kv.split('=', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                \n",
    "                if key.startswith(\"STORE_ID\"):\n",
    "                    continue\n",
    "                try:\n",
    "                    numeric_value = float(value)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if numeric_value > 0:\n",
    "                    transaction.append(key)\n",
    "        dataset.append(transaction)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = process_transaction_csv(filename)\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit_transform(dataset)\n",
    "binary_foodmart = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "binary_foodmart.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(binary_foodmart.columns)) # Number of Different Products\n",
    "len(binary_foodmart) # Number of Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Foodmart Basic Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lines and Columns\n",
    "\n",
    "- Number of Transactions/ Baskets = 69549\n",
    "- Number of Total Product Types (number of columns) = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_foodmart.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Are there empty transactions?\n",
    "\n",
    "Not anymore, before we were having issues due to the problems in the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_foodmart[binary_foodmart.sum(axis=1) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_count = 0\n",
    "for check in dataset:\n",
    "    if check == []: \n",
    "        empty_countcount += 1\n",
    "if empty_count == 0:\n",
    "    print('No empty transactions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Are there empty columns?\n",
    "\n",
    "Just by the way we parsed the binary table it would be impossible to have columns with no transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_columns = binary_foodmart.columns[binary_foodmart.sum(axis=0) == 0]\n",
    "\n",
    "if len(empty_columns) > 0:\n",
    "\tprint(\"Empty columns found:\", empty_columns.tolist())\n",
    "else:\n",
    "\tprint(\"No empty columns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean, maximum, minimum and median products per transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each True is treated as 1\n",
    "product_counts = binary_foodmart.sum(axis=1)\n",
    "\n",
    "average_products = product_counts.mean()\n",
    "max_products = product_counts.max()\n",
    "min_products = product_counts.min()\n",
    "median_products = product_counts.median()\n",
    "\n",
    "# add quantiles: 25th, 75th, and 90th percentiles as examples.\n",
    "q25 = product_counts.quantile(0.25)\n",
    "q75 = product_counts.quantile(0.75)\n",
    "q90 = product_counts.quantile(0.9)\n",
    "\n",
    "print(\"Mean products per transaction:\", average_products)\n",
    "print(\"Maximum products in a transaction:\", max_products)\n",
    "print(\"Minimum products in a transaction:\", min_products)\n",
    "print(\"Median products per transaction:\", median_products)\n",
    "print(\"25th percentile:\", q25)\n",
    "print(\"75th percentile:\", q75)\n",
    "print(\"90th percentile:\", q90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that customers purchase an average of 4.16 distinct products per trip, with a **median basket size of four items**. The central 50% of transactions span between three and six items, and 90% of baskets contain no more than seven items, indicating that very large purchases (up to 13 items) are uncommon. Overall, basket sizes range from a single item to a maximum of thirteen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the relative frequency (%) of each item across all transactions\n",
    "item_counts        = binary_foodmart.sum(axis=0)                   # absolute counts per item\n",
    "n_transactions     = binary_foodmart.shape[0]                     # total number of transactions\n",
    "relative_freq      = (item_counts / n_transactions) * 100         # relative frequency in percentage\n",
    "\n",
    "# Sort items by descending relative frequency\n",
    "relative_freq      = relative_freq.sort_values(ascending=False)\n",
    "\n",
    "# (Optional) Compute the overall average relative frequency across all items\n",
    "avg_rel_freq_all   = relative_freq.mean()\n",
    "\n",
    "# Display the top 10 most‐frequent items and the overall average\n",
    "print(\"Top 10 items by relative frequency (%):\")\n",
    "print(relative_freq.head(10).round(2).to_frame(name=\"RelFreq (%)\"))\n",
    "print(f\"\\nAverage relative frequency across all {len(relative_freq)} items: {avg_rel_freq_all:.2f}%\")\n",
    "\n",
    "# Calculate the relative frequency (%) of each item\n",
    "item_counts       = binary_foodmart.sum(axis=0)\n",
    "relative_freq     = item_counts / len(binary_foodmart) * 100\n",
    "\n",
    "# Compute median relative frequency\n",
    "median_rel_freq   = relative_freq.median()\n",
    "\n",
    "print(f\"Median relative frequency across all {len(relative_freq)} items: {median_rel_freq:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Absolute Item Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = binary_foodmart.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "plot_df = freq.reset_index()\n",
    "plot_df.columns = ['Item', 'Count']\n",
    "\n",
    "fig = px.bar(\n",
    "    plot_df,\n",
    "    x='Item',       # categories on the x-axis\n",
    "    y='Count',      # bar heights\n",
    "    orientation='v',\n",
    "    title='Absolute Frequency of Each Item in Foodmart',\n",
    "    labels={\n",
    "        'Count': 'Number of Purchases',\n",
    "        'Item': 'Product'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=max(50, len(plot_df) * 5),\n",
    "    width=max(1000, len(plot_df) * 20),  # 👈 add width\n",
    "    margin=dict(l=50, r=50, t=50, b=200),\n",
    "    xaxis_tickangle=-45,\n",
    "    xaxis={'categoryorder': 'total descending'}\n",
    ")\n",
    "\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relative Item Frequency / Support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (binary_foodmart.sum(axis=0) / len(binary_foodmart) * 100).sort_values(ascending=False)\n",
    "\n",
    "plot_df = freq.reset_index()\n",
    "plot_df.columns = ['Item', 'Percentage']\n",
    "\n",
    "fig = px.bar(\n",
    "    plot_df,\n",
    "    x='Item',       # categories on the x-axis\n",
    "    y='Percentage', # bar heights\n",
    "    orientation='v',\n",
    "    title='Relative Frequency of Each Item in Foodmart',\n",
    "    labels={\n",
    "        'Percentage': 'Percentage of Transactions (%)',\n",
    "        'Item': 'Product'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_traces(marker_color='seagreen')  # Set bars to green color\n",
    "\n",
    "fig.update_layout(\n",
    "    height= max(50, len(plot_df) * 5),\n",
    "    width=max(1000, len(plot_df) * 20),\n",
    "    margin=dict(l=50, r=50, t=50, b=150),     \n",
    "    xaxis_tickangle=-45,                      \n",
    "    xaxis={'categoryorder':'total descending'} \n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both plots have an extremely **right-skewed item-frequency distribution**, aka a **long-tailed** distribution. A handful of products (the most frequent/ popular items; e.g., *Fresh Vegetables* and *Fresh Fruit*) account for the overwhelming majority of purchases, while the frequency drops off precipitously for the remaining hundreds of items.  \n",
    "\n",
    "But why this is a problem for association mining?\n",
    "\n",
    "Association algorithms such as Apriori or FP-Growth **rely on the support threshold** to prune the search space. With a long tail:\n",
    "\n",
    "* If one sets **`min_support` high enough**, every tail item (and all rules involving them) falls below the cut-off and is silently discarded—even if some of those rules are business-critical niche insights.  \n",
    "* If ones **lowers `min_support`** to save the tail items, the head items generate a combinatorial explosion of frequent itemsets and rules, flooded with spurious or redundant patterns.  \n",
    "\n",
    "Essentially, statistical distortions are unfortunately likely to happen, because head items have such large base rates that they dominate metrics:\n",
    "\n",
    "* **Confidence** can be inflated merely because a consequent is common, not because the antecedent truly implies it. This can be discerned normalizing with lift.\n",
    "* However, to get a good lift (>= 1.2) we'll have to use incredibly low support thresholds. This can be an issue seeing how **Lift** and related interest measures become unstable for low frequencies. **Sampling error** makes their values swing wildly, **producing apparently solid (high values) but unreliable rules**, meaning at low frequencies new purchases added to the dataset could completely flip the produced values (not that reliable). Therefore, **rules generated with low support, high lift** will not be frequent and strong associations, but <ins>rather interest patterns would need further more robust datasets to validate</ins>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Compute Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute frequent itemsets considering a minimum support S_min. \n",
    "* Present frequent itemsets organized by length (number of items). \n",
    "* List frequent 1-itemsets, 2-itemsets, 3-itemsets, etc with support of at least S < S_min.\n",
    "* Change the minimum support values and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# higher support items will appear first; descendent order\n",
    "def compute_frequent_itemset_subsets(df, min_support, algorithm='fp_growth'):\n",
    "    \"\"\"\n",
    "    Compute frequent itemsets from a one-hot encoded DataFrame using a specified algorithm and a minimum support threshold,\n",
    "    and return the frequent itemsets organized by their length as separate DataFrames.\n",
    "    Parameters:\n",
    "    df : pandas.DataFrame\n",
    "        A one-hot encoded DataFrame where each row represents a transaction and each column\n",
    "        represents an item as a boolean value (True/False).\n",
    "    min_support : float\n",
    "        A value between 0 and 1 specifies the minimum fraction of transactions in which an itemset \n",
    "        must appear to be considered frequent.\n",
    "    algorithm: str, optional (default='apriori')\n",
    "        The algorithm to be used for frequent itemset mining. Supported options are 'apriori' and 'fp_growth'.\n",
    "    Returns:\n",
    "        A dictionary mapping each itemset length (e.g., 1, 2, 3, …) to a Ddata fame containing the frequent itemsets\n",
    "        of that length, with their corresponding support values,\n",
    "        ordered in descending order so that itemsets with higher support appear first.\n",
    "    \"\"\"\n",
    "    if algorithm == 'apriori':\n",
    "        frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n",
    "    elif algorithm == 'fp_growth':\n",
    "        frequent_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported algorithm. Please use 'apriori' or 'fp_growth'.\")\n",
    "    \n",
    "    frequent_itemsets['itemset_length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    \n",
    "    # Organize and sort the frequent itemsets by their length.\n",
    "    subsets = {}\n",
    "    for length in sorted(frequent_itemsets['itemset_length'].unique()):\n",
    "        subset_df = frequent_itemsets[frequent_itemsets['itemset_length'] == length]\n",
    "        # Order the itemset subset by support in descending order.\n",
    "        subset_df = subset_df.sort_values(by='support', ascending=False)\n",
    "        subsets[length] = subset_df\n",
    "        \n",
    "    return subsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualise length itemsets with a key that equals their itemset partition length.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.0. What Mining Algorithm to Use?\n",
    "*Much faster using FP-Growth, as seen in the following cells.*\n",
    "\n",
    "As expected, when the minimum support threshold is as low as 0.1%, the <ins>FP-Growth algorithm demonstrates significantly faster performance compared to Apriori</ins> because it leverages an efficient data structure known as the FP tree. \n",
    "\n",
    "Unlike the Apriori algorithm that generates **candidate itemsets**, **FP-growth compacts the representation of the transactional dataset**, minimising the need for exhaustive candidate generation, and allowing the algorithm to bypass the aforementioned combinatorial explosion (maximum items per transaction is 13). \n",
    "\n",
    "Therefore, FP-Growth quickly identifies frequent patterns even amidst numerous potential itemsets, leading to improvements in execution time under such low support conditions. The computational time improvements are comparatively plotted in the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_thresholds = [0.05, 0.04, 0.03, 0.02, 0.01, 0.0075, 0.005]\n",
    "results = []\n",
    "\n",
    "for support in support_thresholds:\n",
    "    start_time = time.time()\n",
    "    _ = compute_frequent_itemset_subsets(binary_foodmart, min_support=support, algorithm='apriori')\n",
    "    apriori_time = time.time() - start_time\n",
    "    results.append({'Support': support * 100, 'Time': apriori_time, 'Algorithm': 'Apriori'})\n",
    "    \n",
    "    start_time = time.time()\n",
    "    _ = compute_frequent_itemset_subsets(binary_foodmart, min_support=support, algorithm='fp_growth')\n",
    "    fpgrowth_time = time.time() - start_time\n",
    "    results.append({'Support': support * 100, 'Time': fpgrowth_time, 'Algorithm': 'FP-Growth'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times = pd.DataFrame(results)\n",
    "df_times['Support_label'] = df_times['Support'].map(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "df_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_order = sorted(df_times['Support_label'].unique(),\n",
    "                       key=lambda s: float(s.strip('%')), reverse=True)\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.lineplot(\n",
    "    data=df_times,\n",
    "    x='Support_label',\n",
    "    y='Time',\n",
    "    hue='Algorithm',\n",
    "    marker='o',\n",
    "    sort=False  \n",
    ")\n",
    "ax.set_title('Execution Time: Apriori vs FP-Growth')\n",
    "ax.set_xlabel('Minimum Support Threshold')\n",
    "ax.set_ylabel('Execution Time (seconds)')\n",
    "ax.set_xticks(support_order)\n",
    "ax.set_xticklabels(support_order)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Minimum Relative Support Threshold\n",
    "\n",
    "**Support** (relative support) is the proportion of transactions in a dataset that contain a specific itemset, and ranges from 0 to 1.  \n",
    "For example, if an itemset appears in 5 out of 10 transactions, its support is 0.5 (50%).\n",
    "\n",
    "$$\n",
    "Support(X) = \\frac{\\text{Number of transactions containing }X}{\\text{Total Number of Transactions}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "supp(A⇒B) = \\frac{|A∪B|}{\\text{Total Number of Transactions}}\n",
    "$$ \n",
    "\n",
    "\n",
    "The **minimum support threshold** is the cutoff that we set:\n",
    "<ins>any itemset with support below this value is discarded</ins>, while those above it are considered ***frequent*** and are used to generate association rules.\n",
    "\n",
    "Nonetheless, it’s important to understand that **support is somewhat of a relative metric**, depending on the total number of transactions in the dataset.\n",
    "\n",
    "- A `0.5%` minimum support means an itemset must appear in **at least ~348 transactions** to be considered frequent.  \n",
    "  This can be a significant threshold, especially for **larger itemsets**, which naturally occur less often.\n",
    "\n",
    "- In contrast, a `0.01%` minimum support means an itemset only needs to appear in **about 7 transactions**.  \n",
    "  This can allow **too many low-frequency or noisy patterns** into the results, which may bloat the output and reduce the overall quality of insights.\n",
    "\n",
    "To choose a practical minimum support, we plotted the number of frequent itemsets against varying thresholds. **As minimum support decreases, the number of patterns typically grows exponentially**. The ***“elbow point”*** in this curve <ins>is where the growth rate sharply increases, suggesting a balanced threshold that captures meaningful patterns</ins> without introducing excessive noise.\n",
    "\n",
    "This elbow marks a sweet spot: \n",
    " * **it captures most of the meaningful patterns** \n",
    " * **while avoiding an explosion (exponential growth) of trivial or noisy itemsets**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup_values = []\n",
    "itemset_counts = []\n",
    "min_occurrences = []\n",
    "\n",
    "n_transactions = len(binary_foodmart)\n",
    "\n",
    "# Generate thresholds from 5.0% to 0.1% in 0.1% steps (i.e., 0.001 in fraction)\n",
    "for min_sup in np.arange(0.05, 0.0009, -0.001):\n",
    "    try:\n",
    "        frequent_itemsets = fpgrowth(binary_foodmart, min_support=min_sup, use_colnames=True)\n",
    "        num_itemsets = len(frequent_itemsets)\n",
    "        min_sup_values.append(round(min_sup * 100, 3))  # Convert to percentage\n",
    "        itemset_counts.append(num_itemsets)\n",
    "        min_occurrences.append(int(np.ceil(min_sup * n_transactions)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error at min_sup = {min_sup*100:.3f}%: {e}\")\n",
    "        break\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Minimum Support (%)': min_sup_values,\n",
    "    'Number of Frequent Itemsets': itemset_counts,\n",
    "    'Min Occurrences': min_occurrences\n",
    "})\n",
    "\n",
    "results_df['Jump'] = results_df['Number of Frequent Itemsets'].diff().fillna(0).astype(int)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow curve\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='Minimum Support (%)', y='Number of Frequent Itemsets', data=results_df, marker='o', color='blue')\n",
    "plt.xlabel('Minimum Support (%)')\n",
    "plt.ylabel('Number of Frequent Itemsets')\n",
    "plt.title('Elbow Plot for binary_foodmart Dataset')\n",
    "plt.gca().invert_xaxis()  # Higher support on the left\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elbow Point Results\n",
    "- **5.0% → 3.0%**: Slow growth (32 → 54 itemsets), capturing only the most frequent patterns.  \n",
    "- **3.0% → 2.0%**: Moderate growth (54 → 77), still manageable.  \n",
    "- **2.0% → 1.5%**: Slight uptick (77 → 97).  \n",
    "- **1.5% → 1.0%**: Noticeable jump (97 → 178), marking the start of steeper growth.  \n",
    "- **1.0% → 0.5%**: More than doubles (178 → 374).  \n",
    "- **0.5% → 0.1%**: Explosive increase (374 → 3009), indicating many rare/noisy itemsets.\n",
    "\n",
    "Based on this, the elbow point appears to be around 1.5% to 0.5%, because this range balances pattern coverage and noise:\n",
    "\n",
    "- **1.5%** yields **97** itemsets— compact, high‑confidence patterns.  \n",
    "- **1.0%** yields **178** itemsets- broader coverage yet still manageable.  \n",
    "- **0.5%** yields **374** itemsets— much broader coverage, may not be manageable.  (probably elbow point!)\n",
    "- <ins>Below 0.5%, itemsets explode, burdening computation and including trivial rules</ins>.\n",
    "\n",
    "**ELBOW POINT SEEMS TO BE 0.5%, TLDR**:\n",
    " * Above 0.5%: you capture the strongest patterns without generating too many rules.\n",
    " * Much below 0.3%: rule counts explode, making interpretation and filtering unwieldy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimum Support Threshold = 5%\n",
    "\n",
    "Each itemset show up in at least ~3478 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_min_high = 0.05\n",
    "frequent_subsets_high = compute_frequent_itemset_subsets(binary_foodmart, S_min_high)\n",
    "print(\"Frequent itemsets can be partioned in\",len(frequent_subsets_high), \"itemsets of differing lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 1 item\")\n",
    "frequent_subsets_high[1].sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 2 items\")\n",
    "frequent_subsets_high[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mininum Support Threshhold = 1%\n",
    "\n",
    "Each itemset show up in at least ~696 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_min_high = 0.01\n",
    "frequent_subsets_high = compute_frequent_itemset_subsets(binary_foodmart, S_min_high)\n",
    "print(\"Frequent itemsets can be partioned in\",len(frequent_subsets_high), \"itemsets of differing lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 1 item\")\n",
    "frequent_subsets_high[1].sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 2 items\")\n",
    "frequent_subsets_high[2].sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimum Support Threshold = 0.5%\n",
    "\n",
    "Each itemset show up in at least ~347 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_min_high = 0.005\n",
    "frequent_subsets_high = compute_frequent_itemset_subsets(binary_foodmart, S_min_high, \"fp_growth\")\n",
    "print(\"Frequent itemsets can be partioned in\",len(frequent_subsets_high), \"itemsets of differing lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 1 item\")\n",
    "frequent_subsets_high[1].sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 2 items\")\n",
    "frequent_subsets_high[2].sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 3 items\")\n",
    "frequent_subsets_high[3].sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimum Support Threshold = 0.1%\n",
    "\n",
    "Each itemset show up in at least ~70 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_min_high = 0.001\n",
    "frequent_subsets_high = compute_frequent_itemset_subsets(binary_foodmart, S_min_high, \"fp_growth\")\n",
    "print(\"Frequent itemsets can be partioned in\",len(frequent_subsets_high), \"itemsets of differing lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 1 items\")\n",
    "frequent_subsets_high[1].sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 2 items\")\n",
    "frequent_subsets_high[2].sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 3 items\")\n",
    "frequent_subsets_high[3].sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Discussing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, setting the minimum support threshold to **5%** yielded limited set of frequent itemsets. Under this criterion, only the strongest individual items emerged as frequent, and only one pair (Fresh Fruit and Fresh Vegetables) was identified as a frequent 2-item set. This outcome reflects the very high and unbalanced frequency of top items like Fresh Vegetables and Fresh Fruit, which appear in approximately **28%** and **17%** of baskets respectively, whereas the average support of individual items is around **4%**.\n",
    "\n",
    "Lowering the threshold to **1%** led to a marked increase in the number of frequent itemsets. The pool of frequent 1-item sets expanded considerably to include a wider range of products, and the number of 2-item sets also rose, thereby revealing more co-occurrence patterns. At this level, each itemset appears in roughly 695 transactions (a nontrivial criterion).\n",
    "\n",
    "A further reduction of the support threshold to **0.5%** produced an even more dramatic rise in the number of frequent itemsets, including the appearance of 3-item sets. Under these conditions, it was captured a diverse array of item combinations, showing more intricate interactions among products. However, **lowering the support this far also increases the likelihood** of incorporating **statistically insignificant patterns**, as the gain of associations began to rise exponentially (noise creeping in).\n",
    "\n",
    "Essentially, setting the support threshold to around **1.5%** or **1%** produces a leaner output. The algorithm returns a focused collection of frequent itemsets while excluding less common patterns. However, the dataset being so unbalanced, a great deal of information is likely lost. Conversely, reducing the threshold to **0.5%** (the detected “***elbow point***”) shows a substantially larger number of itemsets, including higher-order combinations that better represent the domain in question. However, this richer diversity may also introduce additional noise. Some residual noise may later be pruned through further rule-level thresholds, such as confidence and lift cutoffs.\n",
    "\n",
    "Minimum support thresholds just a few decimals below **0.5%** (for example, **0.1%**) are already likely to introduce excessive noise and yield impractically bloated outputs, which is why they were not pursued.\n",
    "\n",
    "<ins>We decided to have <strong>0.5% to 0.3%</strong> as our standard minimum support thresholds, but we still experiment with others to ensure robustness of this decision.</ins> For an itemset to be considered frequent they have to show up at least in:\n",
    "* ~374 transactions (0.5%)\n",
    "* ~208 transactions (0.3%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Generate Association Rules from Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "##### To Do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a minimum support S_min fundamented by the previous results. \n",
    "* Generate association rules with a chosen value (C) for minimum confidence. \n",
    "* Generate association rules with a chosen value (L) for minimum lift. \n",
    "* Generate association rules with both confidence >= C and lift >= L.\n",
    "* Change C and L when it makes sense and discuss the results.\n",
    "* Use other metrics besides confidence and lift.\n",
    "* Evaluate how good the rules are given the metrics and how interesting they are from your point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.0. What are Association Rules?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An association rule implies that if itemset **X** occurs, then itemset **Y** also occurs with a certain probability. It is defined as an implication: `X → Y` where **X** (the _antecedent_) and **Y** (the _consequent_) are disjoint subsets (no item in common) of the full itemset **I** (X, Y ⊆ I and X ∩ Y = ∅).\n",
    "\n",
    "**Example:** \n",
    " * `{Bacon, Egg} → {Milk}`  \n",
    " * `Itemset = {Bacon, Egg, Milk}`\n",
    "\n",
    "There are <ins>various metrics that evaluate the perceived strength of assosciation</ins> between **antecedent** and **consequent**:\n",
    "\n",
    " * Support\n",
    " * Confidence\n",
    " * Lift\n",
    " * Leverage\n",
    " * Conviction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Association Rules for Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confidence**: a measure of <ins>how often items in a rule are found together</ins>. For instance, a confidence of 0.6 means that 60% of the baskets with A also have B.\n",
    "* Confidence helps rank rules by how strong they are in absolute.\n",
    "* But **it does not account for the overall frequency of B** (for that, lift is used). \n",
    "\n",
    "$$\n",
    "\\text{Confidence}(A \\rightarrow B) = \\frac{\\text{Support}(A \\cup B)}{\\text{Support}(A)}\n",
    "$$\n",
    "\n",
    "**Confidence** evaluates the probability that **B** (consequent) will be purchased/ occur if **A** (antecedent) is purchased/ occurs. Ranges from 0 to 1:\n",
    "\n",
    "- **High Confidence (close to 1)**: This means that **B** is very likely to appear in transactions that already contain **A**. This implies a strong association between **A** and **B**.\n",
    "  \n",
    "- **Low Confidence (close to 0)**: This means that **B** is not likely to appear in transactions that contain **A**. In this case, the association between **A** and **B** is weak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules computing them at 6 different support levels\n",
    "support_levels    = [0.05, 0.01, 0.005, 0.003, 0.001, 0.0008, 0.0005, 0.0002]\n",
    "confidence_levels = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "# Sweep FP‑Growth and count rules\n",
    "rules_counts = {sup: [] for sup in support_levels}\n",
    "for sup in support_levels:\n",
    "    freq_itemsets = fpgrowth(binary_foodmart, min_support=sup, use_colnames=True)\n",
    "    for conf in confidence_levels:\n",
    "        rules = association_rules(freq_itemsets, metric=\"confidence\", min_threshold=conf)\n",
    "        rules_counts[sup].append(len(rules))\n",
    "\n",
    "records = []\n",
    "for sup in support_levels:\n",
    "    for conf, cnt in zip(confidence_levels, rules_counts[sup]):\n",
    "        records.append({\n",
    "            'Min Support (%)': f\"{sup*100:.4f}\",\n",
    "            'Confidence': conf,\n",
    "            'Num Rules': cnt\n",
    "        })\n",
    "plot_df = pd.DataFrame(records)\n",
    "\n",
    "#  Plotting the results\n",
    "fig = px.line(\n",
    "    plot_df,\n",
    "    x=\"Confidence\",\n",
    "    y=\"Num Rules\",\n",
    "    facet_col=\"Min Support (%)\",\n",
    "    facet_col_wrap=4,\n",
    "    markers=True,\n",
    "    title=\"Rules vs. Confidence for Different Support Levels\",\n",
    "    labels={\"Num Rules\":\"Number of Rules\"}\n",
    ")\n",
    "\n",
    "# Share axes\n",
    "fig.update_yaxes(matches=\"y\")\n",
    "fig.update_xaxes(matches=\"x\")\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=900,\n",
    "    margin=dict(t=80, b=50, l=50, r=50)\n",
    ")\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1] + \"%\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The six‐panel chart makes clear how dramatically the size of our rule set depends on the support threshold. At **5% support**, there are virtually no rules once you raise confidence above 10%, even at 10% confidence, only a couple of trivial pairs survive. Dropping to **1% support** yields about 80–90 rules at 10% confidence, but they all vanish by 30% confidence, so these are very coarse, generic associations. \n",
    "\n",
    "At **0.5% support**, a healthy pool of roughly 250 rules at 10% confidence is finally seen, which thins to fewer than ten by 30%. This is the classic “elbow” region where there is a balance of coverage and manageability. Pushing support lower (0.1%, 0.08%, 0.05%) unleashes thousands of rules at low confidence but still collapses to zero by 40%–50%, flooding the output with ***low value associations***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules computing them at 6 different support levels\n",
    "support_levels    = [0.05, 0.01, 0.005, 0.003, 0.002, 0.001, 0.0008]\n",
    "confidence_levels = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "# Sweep FP‑Growth and count rules\n",
    "rules_counts = {sup: [] for sup in support_levels}\n",
    "for sup in support_levels:\n",
    "    freq_itemsets = fpgrowth(binary_foodmart, min_support=sup, use_colnames=True)\n",
    "    for conf in confidence_levels:\n",
    "        rules = association_rules(freq_itemsets, metric=\"confidence\", min_threshold=conf)\n",
    "        rules_counts[sup].append(len(rules))\n",
    "        \n",
    "records = []\n",
    "for sup in support_levels:\n",
    "    for conf, cnt in zip(confidence_levels, rules_counts[sup]):\n",
    "        records.append({\n",
    "            'Support (%)': f\"{sup*100:.2f}\",\n",
    "            'Confidence': conf,\n",
    "            'Num Rules': cnt\n",
    "        })\n",
    "plot_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# All in one plot\n",
    "sns.set(style=\"darkgrid\", palette=\"deep\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.lineplot(\n",
    "    data=plot_df,\n",
    "    x=\"Confidence\",\n",
    "    y=\"Num Rules\",\n",
    "    hue=\"Support (%)\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "ax.set_title(\"Number of Association Rules vs. Confidence\\nfor Various Support Levels\")\n",
    "ax.set_ylabel(\"Number of Rules\")\n",
    "ax.set_xlabel(\"Confidence Threshold\")\n",
    "plt.legend(title=\"Min. Support (%)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlaying all six curves in a single plot drives home the same point:  \n",
    "- high‐support curves sit flat near zero,  \n",
    "- ultra‐low support curves skyrocket into the thousands at low confidence,  \n",
    "- only the mid‐range supports (0.5–0.1%) give you a substantial—but still limited—number of rules that disappear quickly as you raise confidence.\n",
    "\n",
    "For a concise, high value rule set choosing **min_support ≈ 0.5%** with **min_confidence 20–30%** (yielding a few dozen solid rules) or, for broader coverage, **min_support ≈ 0.1%** with **conf ≥ 20%** is probably for the better. It results in couple hundred rules that can be **further pruned by lift**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RULE_COLS = [\n",
    "    'antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction'\n",
    "]\n",
    "\n",
    "def generate_rules(\n",
    "    df: pd.DataFrame,\n",
    "    min_support = 0.01,\n",
    "    metric = 'confidence',\n",
    "    min_threshold = 0.1):\n",
    "    \"\"\"\n",
    "    Run FP-Growth + association_rules on a binary-encoded DataFrame.\n",
    "\n",
    "    Requires:\n",
    "    df : pd.DataFrame\n",
    "        One-hot / binary-encoded transactions (columns=items, rows=transactions).\n",
    "    min_support : float\n",
    "        Minimum support for FP-Growth (e.g. 0.01 for 1%).\n",
    "    metric : str, default='confidence'\n",
    "        Which metric to use for filtering rules.  \n",
    "        Options include 'confidence', 'lift', 'leverage', 'conviction'.\n",
    "    min_threshold : float, default=0.1\n",
    "        Minimum threshold for the chosen metric (e.g. 0.1 for 10% confidence).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        Association rules, with columns:\n",
    "        antecedents, consequents, support, count, confidence, lift, leverage, conviction.\n",
    "    \"\"\"\n",
    "    freq_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "    \n",
    "    # association rules\n",
    "    rules = association_rules(\n",
    "        freq_itemsets,\n",
    "        metric=metric,\n",
    "        min_threshold=min_threshold\n",
    "    )\n",
    "    \n",
    "    # add absolute counts\n",
    "    n_trans = df.shape[0]\n",
    "    rules['count'] = rules['support'] * n_trans\n",
    "    \n",
    "    return rules[_RULE_COLS].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Association Rules - Minimum Support Threshold == 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_1   = generate_rules(binary_foodmart, 0.01, metric='confidence', min_threshold=0.2)\n",
    "print(\"Number of rules generated:\", len(rules_1))\n",
    "rules_1.sort_values('confidence', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_1   = generate_rules(binary_foodmart, 0.01, metric='confidence', min_threshold=0.3)\n",
    "print(\"Number of rules generated:\", len(rules_1))\n",
    "rules_1.sort_values('confidence', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Association Rules - Minimum Support Threshold == 0.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_05   = generate_rules(binary_foodmart, 0.005, metric='confidence', min_threshold=0.2)\n",
    "print(\"Number of rules generated:\", len(rules_05))\n",
    "rules_05.sort_values('confidence', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_05   = generate_rules(binary_foodmart, 0.005, metric='confidence', min_threshold=0.3)\n",
    "print(\"Number of rules generated:\", len(rules_05))\n",
    "rules_05.sort_values('confidence', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Association Rules - Minimum Support Threshold == 0.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_01   = generate_rules(binary_foodmart, 0.001, metric='confidence', min_threshold=0.3)\n",
    "print(\"Number of rules generated:\", len(rules_01))\n",
    "rules_01.sort_values('confidence', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_01   = generate_rules(binary_foodmart, 0.001, metric='confidence', min_threshold=0.4)\n",
    "print(\"Number of rules generated:\", len(rules_01))\n",
    "rules_01.sort_values('confidence', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 1.3.1.1. Discussing Results\n",
    "\n",
    "When **min_support = 1%** and **min_confidence = 10%** are applied, FP-Growth generates 89 rules, all predicting **Fresh Vegetables**.  \n",
    "At this threshold, each single-item antecedent (e.g., Shampoo, Donuts, Soup) co-occurs with Fresh Vegetables only marginally more often than expected by chance, resulting in weak, generic associations.  This **phenomenon is attributable to the high support of the consequent: very common items yield trivial rules that inflate confidence without providing substantive rules**.\n",
    "\n",
    "When the confidence threshold is raised to **30%** at the same support level, the rule set is reduced to four specific associations—  \n",
    "Shampoo → Fresh Vegetables\n",
    "Donuts → Fresh Vegetables\n",
    "Personal Hygiene → Fresh Vegetables\n",
    "Dried Fruit → Fresh Vegetables.\n",
    "This outcome confirms that single-item rules at 1% support lack both strength and variety, indicating that the chosen support level may be overly restrictive.\n",
    "\n",
    "When support is lowered to **0.5%**, the rule set expands dramatically:  \n",
    "- At **10% confidence**, 249 rules are discovered, including a multi-item antecedent such as {Soup, Fresh Fruit} → {Fresh Vegetables}.with confidence 34% and lift 1.20. This rule represents a strong, actionable cross-sell insight not observed at the higher support threshold.  \n",
    "- When confidence is increased to **20%** under 0.5% support, 76 rules remain—many involving multiple antecedents.\n",
    "* Finally, at **30%** confidence, cut-off yields only nine rules, primarily those with the highest lift and confidence.\n",
    "\n",
    "The **small-numbers illusion** should also be acknowledged: when support falls below approximately 0.1%, rules can attain high confidence based on only 70 transactions, rendering them highly sensitive to random noise. In such cases, a few additional transaction can dramatically alter confidence values, and spurious associations may arise purely by chance after evaluating thousands of rare item pairs.\n",
    "\n",
    "To fix these brittle patterns, it is **ESSENTIAL** to apply a **lift** filter following the confidence threshold: lift measures the deviation of observed co-occurrence from random expectation and **exposes/removes rules whose impressive confidence is driven by item popularity rather than genuine affinity** (like the Fresh Vegetables consequent issue). Only rules that satisfy both a sensible minimum support and a lift-based screening can be considered robust enough to have some meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Association Rules for Lift (*without confidence*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lift**: evaluate the strength of a rule beyond making sure it's not just by chance. It measures how much more likely two items are to appear together, compared to being independent.\n",
    "* **THE BIG DIFFERENCE BETWEEN LIFE AND CONFIDENCE**:\n",
    "\n",
    "\n",
    "    * **Confidence** measures how likely B is given A, but it does not account for the relative frequencies of A and B. This means that high confidence does not always imply a strong relationship between A and B. <ins>If one of them is incredibly popular it can easily inflate the value, misleading us.</ins> \n",
    "\n",
    "\n",
    "    * **Lift**, on the other hand, <ins>normalizes the support of A and B and provides a true measure of association, factoring in how frequently both items occur in the dataset</ins>, axing trivial associations.\n",
    "* Essentially, lift **corrects for the frequency bias of individual items**.\n",
    "* Helps identify strong correlations in the data that may not be apparent from support alone.\n",
    "\n",
    "$$\n",
    "\\text{Lift}(A \\rightarrow B) = \\frac{\\text{Support}(A \\cup B)}{\\text{Support}(A) \\times \\text{Support}(B)}\n",
    "$$\n",
    "\n",
    "\n",
    "**Interpretation of Lift**, Range 0 to Infinity:\n",
    "\n",
    "- **Lift = 1**: \\( A \\) and \\( B \\) are **independent**, meaning their co-occurrence is purely by chance.\n",
    "- **Lift > 1**: There is a **positive association** between \\( A \\) and \\( B \\), meaning they appear together **more often than expected**.\n",
    "- **Lift < 1**: There is a **negative association** between \\( A \\) and \\( B \\), meaning they appear together **less often than expected**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_levels  = [0.05, 0.01, 0.005, 0.003, 0.001, 0.0008]\n",
    "lift_thresholds = np.arange(1.0, 2.1, 0.1)\n",
    "records = []\n",
    "\n",
    "for sup in support_levels:\n",
    "    freq_itemsets = fpgrowth(binary_foodmart, min_support=sup, use_colnames=True)\n",
    "    rules = association_rules(freq_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "    for lt in lift_thresholds:\n",
    "        cnt = (rules['lift'] >= lt).sum()\n",
    "        records.append({\n",
    "            'Min Support (%)': f\"{sup*100:.4f}\",\n",
    "            'Lift Threshold': lt,\n",
    "            'Number of Rules': cnt\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(records)\n",
    "\n",
    "# 4) Faceted line plot with Plotly\n",
    "fig = px.line(\n",
    "    plot_df,\n",
    "    x=\"Lift Threshold\",\n",
    "    y=\"Number of Rules\",\n",
    "    facet_col=\"Min Support (%)\",\n",
    "    facet_col_wrap=3,\n",
    "    markers=True,\n",
    "    title=\"Rules vs. Lift Threshold for Different Support Levels\",\n",
    "    labels={\"Number of Rules\":\"Number of Rules\"}\n",
    ")\n",
    "\n",
    "# share axes\n",
    "fig.update_xaxes(matches=\"x\")\n",
    "fig.update_yaxes(matches=\"y\")\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=900,\n",
    "    margin=dict(t=80, b=50, l=50, r=50)\n",
    ")\n",
    "# convert annotations to just the support value (as %)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1] + \"%\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_levels  = [0.05, 0.01, 0.005, 0.003]\n",
    "lift_thresholds = np.arange(1.0, 2.1, 0.1)\n",
    "records = []\n",
    "\n",
    "for sup in support_levels:\n",
    "    # mine frequent itemsets at this support\n",
    "    freq_itemsets = fpgrowth(binary_foodmart, min_support=sup, use_colnames=True)\n",
    "    # generate all rules with lift >= 1.0\n",
    "    rules = association_rules(freq_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "    # count how many rules survive each lift threshold\n",
    "    for lt in lift_thresholds:\n",
    "        count = (rules['lift'] >= lt).sum()\n",
    "        records.append({\n",
    "            'Support': f\"{sup*100:.2f}%\",\n",
    "            'Lift Threshold': lt,\n",
    "            'Rule Count': count\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(records)\n",
    "\n",
    "# Plot with seaborn\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"deep\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=plot_df,\n",
    "    x='Lift Threshold',\n",
    "    y='Rule Count',\n",
    "    hue='Support',\n",
    "    marker='o'\n",
    ")\n",
    "plt.title('Number of Association Rules vs. Lift Threshold', fontsize=14)\n",
    "plt.xlabel('Lift Threshold', fontsize=12)\n",
    "plt.ylabel('Number of Rules', fontsize=12)\n",
    "plt.legend(title='Min Support', title_fontsize=12, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Association Rules - Minimum Support Threshold == 1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least in ~696 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_1   = generate_rules(binary_foodmart, 0.01, metric='lift', min_threshold=1.1)\n",
    "print(\"Number of rules generated:\", len(rules_1))\n",
    "rules_1.sort_values('lift', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_1   = generate_rules(binary_foodmart, 0.01, metric='lift', min_threshold=1.15)\n",
    "print(\"Number of rules generated:\", len(rules_1))\n",
    "rules_1.sort_values('lift', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll use the same formatting function using before in the confidence association rules adapted for the lift.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Association Rules - Minimum Support Threshold == 0.5%\n",
    "\n",
    "At least in ~348 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_05   = generate_rules(binary_foodmart, 0.005, metric='lift', min_threshold=1.1)\n",
    "print(\"Number of rules generated:\", len(rules_05))\n",
    "rules_05.sort_values('lift', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_05   = generate_rules(binary_foodmart, 0.005, metric='lift', min_threshold=1.2)\n",
    "print(\"Number of rules generated:\", len(rules_05))\n",
    "rules_05.sort_values('lift', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Association Rules - Minimum Support Threshold == 0.3%\n",
    "\n",
    "At least in ~209 transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_01   = generate_rules(binary_foodmart, 0.003, metric='lift', min_threshold=1.3)\n",
    "print(\"Number of rules generated:\", len(rules_01))\n",
    "rules_01.sort_values('lift', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_01   = generate_rules(binary_foodmart, 0.003, metric='lift', min_threshold=1.35)\n",
    "print(\"Number of rules generated:\", len(rules_01))\n",
    "rules_01.sort_values('lift', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.2.1. Discussing Results\n",
    "\n",
    "Confidence alone exaggerates rules whose consequents (e.g. *Fresh Vegetables*, *Fresh Fruit*) are already common. On the other hand, lift re-normalizes by the marginals and is therefore the cleanest way to expose genuine over-co-occurrence.  \n",
    "Yet, lift has an Achilles’ heel: **it falls rapidly when either side of the rule is very frequent** (exactly the case in **Foodmart** where the median single-item support is **2.86%**, but the top items reach **28%**).\n",
    "\n",
    "\n",
    "\n",
    "**Behaviour across support slices**\n",
    "\n",
    "| Minimum support | Rules with **lift ≥ 1.00** | Rules with **lift ≥ 1.20** |\n",
    "|-----------------|----------------------------|----------------------------|\n",
    "| **5 %** | *0 – 2* trivial rules | *0* |\n",
    "| **1 %** | ~**70** rules, lift tops at **1.16** | **2** symmetric rules (*Soup ↔ Wine*) |\n",
    "| **0.5 %** | **304** rules, lift up to **1.39** | **18** rules survive |\n",
    "| **0.3 %** | **≈900** rules, lift peaks at **1.52** | **12** rules at **lift ≥ 1.40** |\n",
    "\n",
    "\n",
    "At high support (5%) only “basket staples” clear the threshold, and staples tend to co-occur almost at chance, so lift ≈ 1, meaning the items are basically independent.\n",
    "Dropping to 1% brings in moderately popular items, but lift is still capped around **1.16** because those items are too common to create a large over-representation.  \n",
    "Finally, the most interesting region is **0.5% – 0.3%**: items appearing in 200-350 baskets are rare enough to yield a visible lift yet still frequent enough to be reliable. This is where rules such as  \n",
    "\n",
    "* **Deli Salads → Cheese** (lift **1.39**)  \n",
    "* **Soda → Canned Vegetables** (lift **1.29**)  \n",
    "\n",
    "emerge.  Below 0.3 %, lift can climb past **1.5**, but the absolute counts drop under 200 and the risk of sampling noise rises sharply.\n",
    "\n",
    "\n",
    "**Behaviour of our dataset, why are values so darn low...**\n",
    "\n",
    "* **Skewed item popularity**. Because two extremely frequent items dominate many baskets, most candidate pairs inherit large denominators in the lift formula, squeezing the ratio towards **1**. \n",
    "* **Short baskets** (median length = **4**). With few items per transaction, the probability that a medium-frequency item meets another specific medium-frequency item is small; lift therefore grows slowly.  \n",
    "* **Symmetry & dilution.** When you pool five store formats together, real affinities that are strong in a single format are averaged out (Simpson’s paradox), depressing global lift.\n",
    "\n",
    "We have to work knowing this metrics are always relative to the dataset being used. We could also excise the most popular items but that too would cause issues in the validity of the data not really representing the real domain. (we might try that ahead)\n",
    "\n",
    "\n",
    "**What we will do ahead:**\n",
    "\n",
    "1. **Support should remain the primary gatekeeper.** In this data a floor around **0.5 %** (≈ 350 baskets) balances statistical power and coverage; lower than **0.3 %** produces many rules whose leverage is \\< 0.001 (fewer than one extra co-purchase per thousand transactions).  \n",
    "2. **Lift ≥ 1.2 is a reasonable secondary screen.** It eliminates the “staple with staple” noise while retaining rules with measurable incremental value.  \n",
    "3. **Confidence is *not* a helpful discriminator here.** The longest-lived rules (e.g. *Hot Dogs → Pasta* at support 0.3 %) have confidence only **6 – 8 %** but still beat chance by **50 %** in relative terms. Filtering at confidence > 0.2 would discard them.  \n",
    "4. **Segmenting by store type is essential.** Many global lifts never exceed **1.4** because heterogeneity dilutes them; mining the *Deluxe* or *Small Grocery* subsets separately should expose far stronger local affinities.\n",
    "\n",
    "\n",
    "<ins>**TLDR**</ins>: In a large and unbalanced retail dataset, lift uncovers only modest global associations, unless support is allowed to fall near **0.3 %**.  Rely chiefly on a *support + lift* filter; keep confidence low (≈ 0.10) or omit it, and move to store-specific mining to reveal the truly strong patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Association Rules with both chosen Confidence and Lift values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rule_scatter_plotly(\n",
    "    binary_df,\n",
    "    *,\n",
    "    min_support: float = 0.01,\n",
    "    min_confidence: float = 0.50,\n",
    "    min_lift: float = 1.0,\n",
    "    algo: str = \"fpgrowth\",\n",
    "    color_scale: str = \"Reds\",\n",
    "    height: int = 800,\n",
    "    width: int = 1000,\n",
    "    point_size: int = 8,\n",
    "    **px_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactive scatter: support (%) on x-axis, confidence (%) on y-axis,\n",
    "    colour-coded by lift, labelled with absolute rule count.\n",
    "    \"\"\"\n",
    "    miner = fpgrowth if algo.lower() == \"fpgrowth\" else apriori\n",
    "    freq_sets = miner(binary_df, min_support=min_support, use_colnames=True)\n",
    "    rules = association_rules(freq_sets, metric=\"confidence\",\n",
    "                              min_threshold=min_confidence)\n",
    "    rules = rules[rules[\"lift\"] >= min_lift].reset_index(drop=True)\n",
    "\n",
    "    # readable strings for hover\n",
    "    rules[\"antecedents_str\"] = rules[\"antecedents\"].apply(\n",
    "        lambda x: \", \".join(sorted(x)))\n",
    "    rules[\"consequents_str\"] = rules[\"consequents\"].apply(\n",
    "        lambda x: \", \".join(sorted(x)))\n",
    "\n",
    "    n_tx = binary_df.shape[0]\n",
    "    rules[\"support_pct\"] = rules[\"support\"] * 100\n",
    "    rules[\"confidence_pct\"] = rules[\"confidence\"] * 100\n",
    "    rules[\"count\"] = (rules[\"support\"] * n_tx).round().astype(int)\n",
    "\n",
    "    # ----------- PLOT -----------\n",
    "    fig = px.scatter(\n",
    "        rules,\n",
    "        x=\"support_pct\",\n",
    "        y=\"confidence_pct\",\n",
    "        color=\"lift\",                       # colour by lift (swapped)\n",
    "        color_continuous_scale=color_scale,\n",
    "        hover_data={\n",
    "            \"support_pct\": \":.2f\",\n",
    "            \"confidence_pct\": \":.2f\",\n",
    "            \"lift\": \":.2f\",\n",
    "            \"count\": True,\n",
    "            \"antecedents_str\": True,\n",
    "            \"consequents_str\": True,\n",
    "        },\n",
    "        height=height,\n",
    "        width=width,\n",
    "        title=f\"Support vs Confidence for {len(rules)} Rules\",\n",
    "        **px_kwargs,\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=point_size, line=dict(width=0.5, color=\"black\")),\n",
    "    )\n",
    "\n",
    "    # axis labelling & ranges \n",
    "    xmin = min_support \n",
    "    fig.update_xaxes(title_text=\"support (%)\",\n",
    "                     ticksuffix=\"%\")\n",
    "    fig.update_yaxes(title_text=\"confidence (%)\",\n",
    "                    ticksuffix=\"%\")\n",
    "\n",
    "    # colour-bar title\n",
    "    fig.update_layout(coloraxis_colorbar=dict(title=\"lift\"))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_rule_scatter_plotly(\n",
    "        binary_foodmart,\n",
    "        min_support=0.003,\n",
    "        min_confidence=0.10,\n",
    "        min_lift=1.0,\n",
    "        point_size=8,\n",
    "        opacity=0.8\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_rule_scatter_plotly(\n",
    "        binary_foodmart,\n",
    "        min_support=0.003,\n",
    "        min_confidence=0.20,\n",
    "        min_lift=1.20,\n",
    "        point_size=8,\n",
    "        opacity=0.8\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent itemsets\n",
    "min_sup   = 0.003\n",
    "freq_items = fpgrowth(binary_foodmart, min_support=min_sup, use_colnames=True)\n",
    "\n",
    "# confidence\n",
    "min_conf  = 0.20\n",
    "rules     = association_rules(freq_items, metric=\"confidence\", min_threshold=min_conf)\n",
    "\n",
    "# filter for lift ≥ 1.20\n",
    "min_lift  = 1.2\n",
    "strong_rules = rules[rules['lift'] >= min_lift].copy()\n",
    "\n",
    "# absolute counts\n",
    "strong_rules['count'] = strong_rules['support'] * binary_foodmart.shape[0]\n",
    "\n",
    "strong_rules = strong_rules.sort_values(['lift','confidence'], ascending=False)\n",
    "print(f\"Found {len(strong_rules)} rules with support ≥ {min_sup*100:.1f}%, \"\n",
    "      f\"confidence ≥ {min_conf:.2f}, lift ≥ {min_lift:.2f}\\n\")\n",
    "strong_rules[[\n",
    "    'antecedents','consequents',\n",
    "    'support','count',\n",
    "    'confidence','lift',\n",
    "    'leverage','conviction'\n",
    "]].head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perhaps more Interesting Rules - Working around frequent consequents\n",
    "\n",
    "Using lower confidence but higher lifts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent itemsets\n",
    "min_sup   = 0.003\n",
    "freq_items = fpgrowth(binary_foodmart, min_support=min_sup, use_colnames=True)\n",
    "\n",
    "# confidence\n",
    "min_conf  = 0.06\n",
    "rules     = association_rules(freq_items, metric=\"confidence\", min_threshold=min_conf)\n",
    "\n",
    "# filter for lift ≥ 1.20\n",
    "min_lift  = 1.3\n",
    "strong_rules = rules[rules['lift'] >= min_lift].copy()\n",
    "\n",
    "# absolute counts\n",
    "strong_rules['count'] = strong_rules['support'] * binary_foodmart.shape[0]\n",
    "\n",
    "strong_rules = strong_rules.sort_values(['lift','confidence'], ascending=False)\n",
    "print(f\"Found {len(strong_rules)} rules with support ≥ {min_sup*100:.1f}%, \"\n",
    "      f\"confidence ≥ {min_conf:.2f}, lift ≥ {min_lift:.2f}\\n\")\n",
    "strong_rules[[\n",
    "    'antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction'\n",
    "]].head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_rule_scatter_plotly(\n",
    "        binary_foodmart,\n",
    "        min_support=0.003,\n",
    "        min_confidence=0.06,\n",
    "        min_lift=1.30,\n",
    "        point_size=8,\n",
    "        opacity=0.8\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_association_rule_network(\n",
    "    strong_rules,\n",
    "    *,\n",
    "    comp_gap: float = 6.0,\n",
    "    k: float = 3.0,\n",
    "    scale: float = 3.0,\n",
    "    iterations: int = 150,\n",
    "    seed: int = 42,\n",
    "    figsize: tuple = (18, 18),\n",
    "    node_cmap = plt.cm.viridis,\n",
    "    edge_cmap = plt.cm.RdYlBu_r,\n",
    "    with_edge_labels: bool = True,\n",
    "    return_graph: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualise an association-rule network (node size = degree,\n",
    "    edge width = confidence, colour = lift).\n",
    "    \"\"\"\n",
    "    # directed graph\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in strong_rules.iterrows():\n",
    "        a = \", \".join(sorted(row[\"antecedents\"]))\n",
    "        c = \", \".join(sorted(row[\"consequents\"]))\n",
    "        G.add_edge(a, c,\n",
    "                   confidence=row[\"confidence\"],\n",
    "                   lift=row[\"lift\"],\n",
    "                   support=row[\"support\"])\n",
    "\n",
    "    # layout weakly‑connected components on a grid\n",
    "    pos = {}\n",
    "    components = list(nx.weakly_connected_components(G))\n",
    "    cols = int(len(components) ** 0.5) + 1\n",
    "\n",
    "    for i, comp in enumerate(components):\n",
    "        sg = G.subgraph(comp)\n",
    "\n",
    "        sg_pos = nx.spring_layout(\n",
    "            sg, k=k, scale=scale, iterations=iterations, seed=seed\n",
    "        )\n",
    "\n",
    "        r, c = divmod(i, cols)\n",
    "        dx, dy = c * comp_gap, -r * comp_gap\n",
    "\n",
    "        for n, (x, y) in sg_pos.items():\n",
    "            pos[n] = (x + dx, y + dy)\n",
    "\n",
    "    # styling\n",
    "    deg          = dict(G.degree())\n",
    "    node_sizes   = [2000 + deg[n] * 300 for n in G]\n",
    "    node_norm    = Normalize(vmin=min(deg.values()), vmax=max(deg.values()))\n",
    "    node_colors  = [node_cmap(node_norm(deg[n])) for n in G]\n",
    "\n",
    "    edge_lifts   = [G[u][v][\"lift\"] for u, v in G.edges()]\n",
    "    edge_norm    = Normalize(vmin=min(edge_lifts), vmax=max(edge_lifts))\n",
    "    edge_colors  = [edge_cmap(edge_norm(G[u][v][\"lift\"])) for u, v in G.edges()]\n",
    "    edge_widths  = [G[u][v][\"confidence\"] * 5 for u, v in G.edges()]\n",
    "\n",
    "    # Plot \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos, ax=ax,\n",
    "        node_size=node_sizes,\n",
    "        node_color=node_colors,\n",
    "        edgecolors=\"white\",\n",
    "        linewidths=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos, ax=ax,\n",
    "        width=edge_widths,\n",
    "        edge_color=edge_colors,\n",
    "        arrowsize=22,\n",
    "        arrowstyle=\"->\",\n",
    "        connectionstyle=\"arc3,rad=0.2\",\n",
    "        alpha=0.85,\n",
    "    )\n",
    "    nx.draw_networkx_labels(\n",
    "        G, pos, ax=ax,\n",
    "        font_size=10,\n",
    "        font_weight=\"bold\",\n",
    "        bbox=dict(facecolor=\"white\", edgecolor=\"none\", pad=0.5),\n",
    "    )\n",
    "\n",
    "    if with_edge_labels:\n",
    "        edge_lbl = {\n",
    "            (u, v): f\"conf:{G[u][v]['confidence']:.2f}\\nlift:{G[u][v]['lift']:.2f}\"\n",
    "            for u, v in G.edges()\n",
    "        }\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            G, pos, ax=ax,\n",
    "            edge_labels=edge_lbl,\n",
    "            font_size=8,\n",
    "            bbox=dict(facecolor=\"white\", edgecolor=\"none\", pad=0.2),\n",
    "        )\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap=edge_cmap, norm=edge_norm)\n",
    "    sm.set_array([])\n",
    "    fig.colorbar(sm, ax=ax, label=\"Lift\", fraction=0.046, pad=0.04)\n",
    "\n",
    "    ax.set_title(\n",
    "        \"Association-Rule Network\\n\"\n",
    "        \"(node size = degree, edge width = confidence, colour = lift)\",\n",
    "        fontsize=15, pad=20,\n",
    "    )\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if return_graph:\n",
    "        return fig, G, pos\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, G, pos = plot_association_rule_network(strong_rules, return_graph=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_05   = generate_rules(binary_foodmart, 0.005, metric='leverage', min_threshold=0.0018)\n",
    "print(\"Number of rules generated:\", len(rules_05))\n",
    "rules_05.sort_values('lift', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_05   = generate_rules(binary_foodmart, 0.005, metric='conviction', min_threshold=1.05)\n",
    "print(\"Number of rules generated:\", len(rules_05))\n",
    "rules_05.sort_values('lift', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TEST REMOVING TOP ITEMS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 most frequent items using the 'freq' Series\n",
    "top_10_freq = freq.head(5)\n",
    "print(\"Top 10 most frequent items by percentage of transactions:\")\n",
    "print(top_10_freq)\n",
    "\n",
    "# If you need just the list of item names:\n",
    "top_10_items = top_10_freq.index.tolist()\n",
    "print(\"\\nTop 10 items list:\")\n",
    "print(top_10_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of items per transaction\n",
    "product_counts = binary_foodmart.sum(axis=1)\n",
    "\n",
    "# Get descriptive statistics\n",
    "desc_stats = product_counts.describe()\n",
    "q1 = product_counts.quantile(0.25)\n",
    "median = product_counts.quantile(0.50)\n",
    "q3 = product_counts.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "mean = product_counts.mean()\n",
    "std = product_counts.std()\n",
    "\n",
    "print(\"Descriptive Statistics:\\n\", desc_stats)\n",
    "print(\"\\nQuartiles and Metrics:\")\n",
    "print(\"Q1 (25th percentile):\", q1)\n",
    "print(\"Median (50th percentile):\", median)\n",
    "print(\"Q3 (75th percentile):\", q3)\n",
    "print(\"Interquartile Range (IQR):\", iqr)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the two dominant items\n",
    "cols_to_drop = ['Fresh Vegetables', 'Fresh Fruit', 'Soup']\n",
    "df_stripped   = binary_foodmart.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Remove transactions that became empty after the drop\n",
    "mask_non_empty   = df_stripped.sum(axis=1) > 0          # at least one “True”\n",
    "df_stripped      = df_stripped[mask_non_empty].reset_index(drop=True)\n",
    "\n",
    "# quick sanity-check\n",
    "print(f\"Shape after cleaning : {df_stripped.shape}\")\n",
    "print(f\"Empty-basket rows    : {(df_stripped.sum(axis=1)==0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows in df_stripped with fewer than 3 True values (i.e., less than 3 items)\n",
    "df_stripped = df_stripped[df_stripped.sum(axis=1) >= 3]\n",
    "print(\"Remaining transactions:\", df_stripped.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent itemsets\n",
    "min_sup   = 0.005\n",
    "freq_items = fpgrowth(df_stripped, min_support=min_sup, use_colnames=True)\n",
    "\n",
    "# confidence\n",
    "min_conf  = 0.10\n",
    "rules     = association_rules(freq_items, metric=\"confidence\", min_threshold=min_conf)\n",
    "\n",
    "# Filter for lift ≥ 1.10\n",
    "min_lift  = 1.1\n",
    "strong_rules = rules[rules['lift'] >= min_lift].copy()\n",
    "\n",
    "# absolute counts\n",
    "strong_rules['count'] = strong_rules['support'] * df_stripped.shape[0]\n",
    "\n",
    "strong_rules = strong_rules.sort_values(['confidence'], ascending=False)\n",
    "print(f\"Found {len(strong_rules)} rules with support ≥ {min_sup*100:.1f}%, \"\n",
    "      f\"confidence ≥ {min_conf:.2f}, lift ≥ {min_lift:.2f}\\n\")\n",
    "strong_rules[[\n",
    "    'antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction'\n",
    "]].head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to double-check quickly:\n",
    "before = 69549\n",
    "after  = 68624\n",
    "print(f\"Basket loss : {(before-after):,}  ({(before-after)/before:.2%})\")\n",
    "\n",
    "before_rules = 22\n",
    "after_rules  = 20\n",
    "print(f\"Rule loss   : {before_rules-after_rules}  \"\n",
    "      f\"({(before_rules-after_rules)/before_rules:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_rules(rules, n_transactions):\n",
    "    \"\"\"\n",
    "    Given the `rules` DataFrame produced by `mlxtend.frequent_patterns.association_rules`\n",
    "    and the total number of transactions in the market-basket matrix (`n_transactions`),\n",
    "    add columns that make the numeric meaning of *leverage* and *conviction* explicit.\n",
    "\n",
    "    New columns\n",
    "    -----------\n",
    "    exp_support   : expected P(A∪B) under independence  = support(A) · support(B)\n",
    "    rel_gain_pct  : relative gain of the pair vs. chance = leverage / exp_support  (in %)\n",
    "    extra_pairs   : absolute # of 'extra' co-occurrences = leverage · n_transactions\n",
    "    \"\"\"\n",
    "    # work on a copy\n",
    "    df = rules.copy()\n",
    "\n",
    "    # remember what was there\n",
    "    original_cols = list(df.columns)\n",
    "\n",
    "    # compute new interpretability columns\n",
    "    df[\"exp_support\"]  = df[\"support\"] / df[\"lift\"]\n",
    "    df[\"rel_gain_pct\"] = (df[\"leverage\"] / df[\"exp_support\"]) * 100\n",
    "    df[\"extra_pairs\"]  = (df[\"leverage\"] * n_transactions).round().astype(int)\n",
    "\n",
    "    # now return only the original + the three new ones, in order\n",
    "    extra_cols = [\"exp_support\", \"rel_gain_pct\", \"extra_pairs\"]\n",
    "    return df[original_cols + extra_cols]\n",
    "\n",
    "\n",
    "n_baskets = df_stripped.shape[0]\n",
    "\n",
    "# enrich and sort for demonstration\n",
    "rules_pretty = (\n",
    "    enrich_rules(strong_rules.copy(), n_baskets)\n",
    "      .sort_values([\"lift\", \"confidence\"], ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# show the top 10 rules with the new interpretability columns\n",
    "pd.set_option(\"display.float_format\", \"{:,.6f}\".format)\n",
    "display(rules_pretty.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully understand the results we also have to get what **Leverage** and **Conviction** are:\n",
    "\n",
    "**Leverage** (also known as *Piatetsky‑Snapiro* measure)  \n",
    "Computes the difference between the observed joint frequency of **X** and **Y** and the frequency we would expect if **X** and **Y** were independent:\n",
    "\n",
    "`Leverage(X → Y) = support(X ∪ Y) − support(X) × support(Y)`\n",
    "\n",
    "- **Range:** [−1, 1]  \n",
    "- **Interpretation:**  \n",
    "  - <ins>Leverage > 0</ins> means **X** and **Y** co‑occur more often than expected (positive correlation).  \n",
    "  - <ins>Leverage < 0</ins> means they co‑occur less than expected (negative correlation).  \n",
    "  - <ins>Leverage = 0</ins> indicates statistical independence.\n",
    "\n",
    "\n",
    "**Conviction**  \n",
    "Measures how often **X** occurs without **Y** (i.e. the rule “fails”) compared to the failure rate if **X** and **Y** were independent:\n",
    "\n",
    "`Conviction(X → Y) = (1 − support(Y)) / (1 − confidence(X → Y))`\n",
    "\n",
    "- **Range:** [0, ∞)  \n",
    "- **Interpretation:**  \n",
    "  - <ins>Conviction > 1</ins> means **Y** is more likely when **X** is present (fewer failures than expected).  \n",
    "  - <ins>Conviction = 1</ins> indicates independence.  \n",
    "  - As confidence → 1, conviction → ∞ (perfect implication)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.3.1. Discussing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **22 rules** survive the triple filter **support ≥ 0.30%**, **confidence ≥ 0.06**, **lift ≥ 1.30**.  \n",
    "* **Counts** run from **211 – 472 baskets** (≈ 0.30–0.69% of the 69 549 transactions).  \n",
    "* **Lift** peaks at **1.52** and bottoms at **1.31** – well above independence (1.0) but far from “basket-lock” strength (> 2).  \n",
    "* **Confidence** sits between **6% and 16%**: modest, yet respectable given the average basket holds only four items.  \n",
    "* **Leverage** hovers around **0.001 ± 0.0003**; conviction around **1.02–1.05**. It's typical for sparse, low-overlap data and tells us the rules add incremental (not transformational) predictive power.\n",
    "\n",
    "**Rules we got rules that seem valid in real world scenarios**:\n",
    "\n",
    "| Most-lifted pair                                                                                                                                     | Intuition                                                                                                         |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|\n",
    "| **Hot Dogs ⇆ Pasta** (lift 1.52)                                                                                                                      | *quick-meal* aisle and share a “family-dinner-in-a-hurry” mission. Hot dogs with pasta combo                       |\n",
    "| **Deli Salads ⇆ Juice / Pizza / Chips / Sliced Bread / Cheese** (five separate rules, lift 1.36–1.52)                                                  | *Deli Salads* act as a picnic / snacking hub that attracts multiple complements; every consequent is grab-and-go     |\n",
    "| **Waffles ⇆ Frozen Chicken** (lift 1.49)                                                                                                              | Classic “chicken-and-waffles” comfort combo.                                                                       |\n",
    "| **Pizza ⇆ Ice-Cream** (lift 1.48)                                                                                                                     | Weekend movie-night basket. Also dope combo.                                                                               |\n",
    "| **Cleaners → Chocolate Candy** (lift 1.43, confidence 9.5 %)                                                                                          | No idea maybe getting a sweet treat to comfort after tidying up their home. Maybe related to impulse buying, item quantity would probably help.                                                                  |\n",
    "| **Canned-Fruit → Canned-Vegetables** & **Cold-Remedies → Canned-Vegetables**                                                                           | Pantry-stocking behaviour; when shoppers buy shelf-stable fruit or cold medicine they also grab veggies.           |\n",
    "| **Nuts ⇆ Soda**                                                                                                                                       | Salty snack + carbonated drink pairing.                                                                            |\n",
    "\n",
    "***Symmetry is important!!!*** 10 of the 22 rules appear in both directions (A → B and B → A). That symmetry, plus similar confidence values, shows the relationships are *co-selection* rather than true causality.\n",
    "\n",
    "\n",
    "**Why leverage & conviction look tiny** \n",
    "Leverage is an *absolute* difference (≈ 0.001 = 0.1 %), so even “good” rules feel numerically small in a 0 – 1 range. The presented values are roughly **100–200 % above random expectation** (e.g., 0.0010 vs 0.0005 baseline), making them meaningful that is meaningful.  \n",
    "Conviction barely exceeds one because confidence never approaches unity in four-item baskets; conviction > 1 nevertheless confirms the rules fail less often than independence would predict, so our rules are still valid.\n",
    "\n",
    "When you mined directly on **leverage ≥ 0.0018** you got only **4 rules**. Essentially the same *Deli-Salads ⇆ Cheese* pair plus a *Fresh-Veg ⇆ Dried-Fruit* artefact (high frequency, low lift). A **conviction ≥ 1.05** cut retained 3 rules and again highlighted *Deli-Salads → Cheese*.  \n",
    "Again this reinforces that sparse grocery data, leverage/conviction are much stricter than lift, they isolate *one or two flagship affinities* rather than a portfolio of cross-sell ideas.\n",
    "\n",
    "\n",
    "##### Why we think the thresholds sensible \n",
    "* **Support 0.30 %** (~208 baskets) is well above the random-pair expectation (~110 for two 4 % items), so the rules are unlikely to be noise.  \n",
    "* **Confidence 6 %** is low in absolute terms, but it is **1.5× the random co-occurrence probability** in a 4-item basket, so it still adds predictive lift.  \n",
    "* **Lift 1.30+** lines up with the 95 % confidence envelope for real association in data of this sparsity. Higher cut-offs (1.5, 1.7) quickly shrink the rule set to < 5 rules.\n",
    "\n",
    "Thus, the chosen triple filter achieves a balance: enough rules to act on, yet few enough to inspect manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Take a Look at Maximal Patterns: Compute Maximal Frequent Itemsets\n",
    "- discuss their utility compared to frequent patterns\n",
    "- analyse the association rules they can unravel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use FP-Max, a variant of FP-Growth, which focuses on obtaining **maximal itemsets**. An itemset X is said to maximal if X is frequent and there exists no frequent super-pattern containing X. In other words, a frequent pattern X cannot be sub-pattern of larger frequent pattern to qualify for the definition maximal itemset (**frequent maximal pattern has no frequent superset**).\n",
    "\n",
    "While the complete set of frequent itemsets can be exponentially large, maximal itemsets provide a **compact representation** by <ins>storing only the largest patterns that meet the support threshold</ins>.  \n",
    "\n",
    "Any non-maximal frequent itemset must be a subset of at least one maximal itemset, so the **maximal collection implies all smaller frequent patterns**. (In the output table ahead, that's why we have to compute unions of each of the items of each itemset after)\n",
    "\n",
    "<ins>Yet, it sacrifices the exact support information for the subsets that are pruned away.</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_min = 0.003\n",
    "max_itemsets = fpmax(binary_foodmart, min_support=S_min, use_colnames=True)\n",
    "max_itemsets[\"length\"] = max_itemsets[\"itemsets\"].apply(len)\n",
    "\n",
    "print(f\"Number of maximal frequent item‑sets (support ≥ {S_min*100:.1f}%): \"\n",
    "      f\"{len(max_itemsets)}\")\n",
    "display(max_itemsets.sort_values(\"length\", ascending=False).head(10))\n",
    "\n",
    "\n",
    "# Generate rules using the full frequent‑itemset table\n",
    "min_conf = 0.06\n",
    "min_lift = 1.3\n",
    "\n",
    "# mine all frequent itemsets at the same support\n",
    "freq_items = fpgrowth(binary_foodmart, min_support=S_min, use_colnames=True)\n",
    "\n",
    "# generate all rules above the confidence threshold\n",
    "all_rules = association_rules(freq_items,\n",
    "                              metric=\"confidence\",\n",
    "                              min_threshold=min_conf)\n",
    "\n",
    "# compute the union of antecedents+consequents row‑wise\n",
    "all_rules[\"union\"] = all_rules.apply(\n",
    "    lambda row: row[\"antecedents\"] | row[\"consequents\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# keep rules whose union is contained in any maximal pattern\n",
    "def is_in_maximal(union_set):\n",
    "    return any(union_set.issubset(m) for m in max_itemsets[\"itemsets\"])\n",
    "\n",
    "rules_max = all_rules[all_rules[\"union\"].apply(is_in_maximal)].copy()\n",
    "\n",
    "# filter on lift and add absolute counts\n",
    "rules_max = rules_max[rules_max[\"lift\"] >= min_lift]\n",
    "rules_max[\"count\"] = (rules_max[\"support\"] * len(binary_foodmart)).astype(int)\n",
    "rules_max = rules_max.sort_values([\"lift\", \"confidence\"], ascending=False)\n",
    "\n",
    "print(f\"\\nRules from maximal itemsets (confidence ≥ {min_conf:.2f}, \"\n",
    "      f\"lift ≥ {min_lift:.2f}): {len(rules_max)}\")\n",
    "display(rules_max[[\n",
    "    'antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction'\n",
    "]\n",
    "].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using <ins>**FP-Max**</ins> at the same **0.3%** support threshold shrinks the pattern catalogue from about **3 600** ordinary frequent itemsets to **679** maximal ones, yet the association-rule phase is unchanged: with <ins>confidence ≥ **0.06**</ins> and <ins>lift ≥ **1.30**</ins> we still recover **22** rules.\n",
    "\n",
    "Every pair that forms a rule is already contained in at least one maximal 3-item set, so discarding smaller subsets does **not** remove any potential rule material.\n",
    "\n",
    "**What changes was basically storage and inspection effort**, support values for pruned subsets disappear, although they can be recomputed on demand, while most of the information is identical. The dominant maximal patterns remain the triads **“Fresh Vegetables + Fresh Fruit + X”**, showing that these two staples underpin most high-support baskets and, consequently, most medium-strength rules. \n",
    "\n",
    "Maximal mining is useful to produce a <ins>*leaner*</ins> yet information, preserving summary of the dataset. **However**, that saving comes at a price: the algorithm jettisons the **exact support for every proper subset**, so details between, for instance, *Cereal & Milk* and *Cereal & Cookies* are removed, and any rerun with tighter confidence/lift bars demands another pass through the data. In short, <ins>maximal mining compresses</ins> the lattice but cannot fully replace the some details that are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Conclusions from Mining Frequent Patterns in All Stores (Global Patterns and Rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global Foodmart basket set contains 69 549 transactions and 102 distinct products, but it is highly skewed—Fresh Vegetables alone appears in 28.45% of baskets and Fresh Fruit in 17.53%. After testing support levels from 5% down to 0.1%, the “elbow” analysis shows that 0.5–0.3% strikes the best balance: it retains statistically sound patterns (≥ ≈208 baskets) yet avoids the explosive growth of noisy itemsets seen below 0.3%. At this floor, FP-Growth vastly out-performs Apriori in run-time and delivers a stable catalogue of ≈ 800 frequent itemsets, from which the triple filter <ins>support ≥ 0.3%, confidence ≥ 0.06, lift ≥ 1.30</ins> distils 22 actionable rules. These rules (e.g. Hot Dogs ⇆ Pasta, Deli Salads ⇆ Juice/Pizza/Chips, Waffles ⇆ Frozen Chicken) all rise 30–52 % above random expectation, add 70-80 “extra” co-purchases per rule, and survive even after the dominant produced items are removed (rule count drops only from 22 to 20).\n",
    "\n",
    "These subdued metrics most likely stem from three structural points of the dataset:\n",
    "- First, the **median basket** holds only **4** items (very small, the 75th percentile being 6 items), so even genuinely related goods co-occur infrequently, depressing confidence. \n",
    "- Second, **item popularity is highly skewed**: staples such as *Fresh Vegetables* appear in 28.45% of transactions, inflating the expected joint probability and pulling lift back toward **1**. \n",
    "- Third, the analysis **mixes five store categories** and ID: <ins>real affinities that are strong in one context are averaged with places where they are absent</ins>, a classic Simpson-paradox dilution.\n",
    "\n",
    "Consequently, the global model uncovers only medium-strength, widely shared habits, *Hot Dogs ⇆ Pasta*, *Deli Salads ⇆ Cheese*, *Pizza ⇆ Ice-Cream*, while sharper, locally valuable relationships remain hidden. To enforce those stronger signals the data must be re-mined in within perhaps more homogeneous slices, for example by store category like we will do ahead. \n",
    "\n",
    "A test was also performed on the three most frequent staples—Fresh Vegetables, Fresh Fruit, and Soup. After removing their columns, discarding baskets that became empty, and retaining only transactions with ≥ 3 items, the dataset shrank from 69 549 → 47 504 transactions and 102 → 99 products while the median basket width remained four. Re-mining with support ≥ 0.5 %, confidence ≥ 0.10, lift ≥ 1.10 returned just 8 rules, led by deli-and-pantry pairings such as Deli Salads → Cheese (18 % confidence, 1.23 lift) and Beer → Dried Fruit (16 %, 1.10). Therefore, eliminating the high-frequency produce items and removing small baskets seem to produce residual rules highlight narrower affinities. This may be useful to work with the tail items.\n",
    "\n",
    "\n",
    "Only after that segmentation might the metrics reflect the true strength of shopper intent rather than the averaging effect of the full **69 549**.\n",
    "\n",
    "Switching to <ins>FP-Max</ins> compresses the pattern lattice from roughly 3 600 to 679 maximal itemsets without losing a single qualifying rule, confirming that maximal mining is an efficient summary tool—but one that discards exact subset supports and therefore cannot answer finer-grained questions without a re-mine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mining Frequent Itemsets and Association Rules: Looking for Differences between Stores\n",
    "\n",
    "The 24 stores, whose transactions were analysed in Task 1, are in fact from purchases carried out in **different types of stores**:\n",
    "* Deluxe Supermarkets: STORE_ID = 8, 12, 13, 17, 19, 21\n",
    "* Gourmet Supermarkets: STORE_ID = 4, 6\n",
    "* Mid-Size Grocerys: STORE_ID = 9, 18, 20, 23\n",
    "* Small Grocerys: STORE_ID = 2, 5, 14, 22\n",
    "* Supermarkets: STORE_ID = 1, 3, 7, 10, 11, 15, 16\n",
    "\n",
    "In this context, in this second task you should compute frequent itemsets and association rules for specific groups of stores (specific/local patterns), and then compare the store specific results with those obtained when all transactions were analysed independently of the type of store (global patterns). \n",
    "\n",
    "**The goal is to find similarities and differences in buying patterns according to the types of store. Do popular products change? Are there buying patterns specific to the type of store?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Analyse Deluxe Supermarkets and Gourmet Supermarkets\n",
    "\n",
    "Here you should analyse **both** the transactions from **Deluxe Supermarkets (STORE_ID = 8, 12, 13, 17, 19, 21)** and **Gourmet Supermarkets (STORE_ID = 4, 6)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Load/Preprocess the Dataset\n",
    "\n",
    "**You might need to change a bit the preprocessing, although most of it should be reused.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transactions(filename):\n",
    "    \"\"\"\n",
    "    Reads the raw CSV, inserts missing commas between numeric alphabetic runs,\n",
    "    parses STORE_ID and product=quantity pairs, and returns a DataFrame with:\n",
    "      • STORE_ID (unlike the prior extraction function)\n",
    "      • items (List[str]) — only products with quantity>0\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "        \n",
    "    # fix lines where digits run directly into letters (e.g. \"Soup=1FreshVegetables\")\n",
    "    fixed = re.sub(r'(?<=[0-9])(?=[A-Za-z])', ',', raw)\n",
    "    reader = csv.reader(io.StringIO(fixed), delimiter=',')\n",
    "\n",
    "    records = []\n",
    "    for row in reader:\n",
    "        rec = {'STORE_ID': None, 'items': []}\n",
    "        for kv in row:\n",
    "            if '=' not in kv:\n",
    "                continue\n",
    "            key, val = kv.split('=', 1)\n",
    "            key = key.strip()\n",
    "            try:\n",
    "                # STORE_ID field\n",
    "                if key == \"STORE_ID\":\n",
    "                    rec['STORE_ID'] = int(val)\n",
    "                # product fields\n",
    "                elif float(val) > 0:\n",
    "                    rec['items'].append(key)\n",
    "            except ValueError:\n",
    "                # ignore any malformed entries\n",
    "                continue\n",
    "        records.append(rec)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "filename = \"Foodmart_2025_DM.csv\"\n",
    "df_all = load_transactions(filename)\n",
    "df_all.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transactions with no-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all['STORE_ID'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping them off**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop transactions with no STORE_ID\n",
    "df_all = df_all.dropna(subset=['STORE_ID']).copy()\n",
    "\n",
    "# (Optional) if STORE_ID was float after dropna, cast to int\n",
    "df_all['STORE_ID'] = df_all['STORE_ID'].astype(int)\n",
    "\n",
    "# Define Deluxe and Gourmet store ID lists\n",
    "deluxe_ids  = [8, 12, 13, 17, 19, 21]\n",
    "gourmet_ids = [4, 6]\n",
    "\n",
    "# Subset transactions by store type\n",
    "deluxe_df  = df_all[df_all['STORE_ID'].isin(deluxe_ids)]\n",
    "gourmet_df = df_all[df_all['STORE_ID'].isin(gourmet_ids)]\n",
    "\n",
    "\n",
    "# Report counts\n",
    "print(f\"Dropped transactions with missing STORE_ID. Remaining rows: {len(df_all)}\")\n",
    "print(f\"Deluxe Supermarkets: {len(deluxe_df)} transactions\")\n",
    "print(f\"Gourmet Supermarkets: {len(gourmet_df)} transactions\")\n",
    "\n",
    "# Verify unique IDs in each subset\n",
    "print(\"Unique store IDs in Deluxe subset:\",  sorted(deluxe_df['STORE_ID'].unique()))\n",
    "print(\"Unique store IDs in Gourmet subset:\", sorted(gourmet_df['STORE_ID'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Into Binary Sparse Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_deluxe  = deluxe_df['items'].tolist()\n",
    "transactions_gourmet = gourmet_df['items'].tolist()\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te.fit(df_all['items'].tolist())\n",
    "\n",
    "# transform each subset\n",
    "te_ary_deluxe  = te.transform(transactions_deluxe)\n",
    "te_ary_gourmet = te.transform(transactions_gourmet)\n",
    "\n",
    "# binary\n",
    "binary_deluxe  = pd.DataFrame(te_ary_deluxe,  columns=te.columns_)\n",
    "binary_gourmet = pd.DataFrame(te_ary_gourmet, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_df =  df_all[df_all['STORE_ID'] == 8]\n",
    "\n",
    "transactions_8 = eight_df['items'].tolist()\n",
    "\n",
    "te_ary_8 = te.transform(transactions_8)\n",
    "\n",
    "binary_8  = pd.DataFrame(te_ary_8,  columns=te.columns_)\n",
    "\n",
    "binary_8.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_stats(name, df):\n",
    "    n_trans = len(df)\n",
    "    n_columns = df.shape[1]\n",
    "    basket_sizes = df.sum(axis=1)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Transactions       = {n_trans:,}\")\n",
    "    print(f\"  Columns            = {n_columns}\")\n",
    "    print(f\"  Avg items/basket   = {basket_sizes.mean():.2f}\")\n",
    "    print(f\"  Median items/basket= {basket_sizes.median():.0f}\")\n",
    "    print()\n",
    "\n",
    "dataset_stats(\"Deluxe Supermarkets\", binary_deluxe)\n",
    "dataset_stats(\"Gourmet Supermarkets\", binary_gourmet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute counts per item\n",
    "count_deluxe = binary_deluxe.sum(axis=0).sort_values(ascending=False)\n",
    "count_gourmet = binary_gourmet.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "df_counts = pd.DataFrame({\n",
    "    'Item': count_deluxe.index,\n",
    "    'Deluxe': count_deluxe.values,\n",
    "    'Gourmet': count_gourmet.reindex(count_deluxe.index, fill_value=0).values\n",
    "})\n",
    "\n",
    "plot_df = df_counts.melt(\n",
    "    id_vars='Item',\n",
    "    value_vars=['Deluxe', 'Gourmet'],\n",
    "    var_name='Store Type',\n",
    "    value_name='Count'\n",
    ")\n",
    "\n",
    "# plot\n",
    "fig = px.bar(\n",
    "    plot_df,\n",
    "    x='Item',\n",
    "    y='Count',\n",
    "    color='Store Type',\n",
    "    barmode='group',\n",
    "    color_discrete_map={'Deluxe':'orange','Gourmet':'violet'},\n",
    "    title='Absolute Transaction Counts by Item: Deluxe vs Gourmet',\n",
    "    labels={'Count':'Number of Transactions','Item':'Product'}\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    height=600,\n",
    "    margin=dict(l=50, r=50, t=60, b=150)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Item Relative Frequencies / Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute relative frequencies\n",
    "freq_deluxe = (binary_deluxe.sum(axis=0) / len(binary_deluxe) * 100).sort_values(ascending=False)\n",
    "freq_gourmet = (binary_gourmet.sum(axis=0) / len(binary_gourmet) * 100).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Item': freq_deluxe.index,\n",
    "    'Deluxe': freq_deluxe.values,\n",
    "    'Gourmet': freq_gourmet.reindex(freq_deluxe.index, fill_value=0).values\n",
    "})\n",
    "\n",
    "plot_df = df.melt(\n",
    "    id_vars='Item',\n",
    "    value_vars=['Deluxe','Gourmet'],\n",
    "    var_name='Store Type',\n",
    "    value_name='Percentage'\n",
    ")\n",
    "\n",
    "# grouped bar plot\n",
    "fig = px.bar(\n",
    "    plot_df,\n",
    "    x='Item',\n",
    "    y='Percentage',\n",
    "    color='Store Type',\n",
    "    barmode='group',\n",
    "    color_discrete_map={'Deluxe':'orange','Gourmet':'violet'},\n",
    "    title='Relative Frequency of Items: Deluxe vs Gourmet',\n",
    "    labels={'Percentage':'Percentage of Transactions (%)','Item':'Product'}\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    height=600,\n",
    "    margin=dict(l=50, r=50, t=60, b=150)\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have the ***\"long tail\"*** distribution in these 2 subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the deluxe supermarkets have a much higher absolute count of transactions, and thus, so of items. This significantly higher number of Deluxe supermarkets transactions (25,923) compared to Gourmet Supermarkets (5,328) indicates the Deluxe dataset is **more robust** and could provide *more reliable* patterns due to its larger sample size. However, the Gourmet dataset's smaller size may result in less statistically significant associations or patterns.\n",
    "\n",
    "\n",
    "The basket size between the two is very similar, suggesting that the client's behavior **is equal** in both type of markets, with the relative frequency of each type of supermarket furtherly confirm it. Though the average number of items bought (4) is small and can hinder the depth of association pattern mining.\n",
    "\n",
    "*Let's see how things roll.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Compute Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This should be trivial now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing Elbow points due to the Long Tail distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vals = np.arange(0.05, 0.0009, -0.001)\n",
    "\n",
    "# frequent itemsets at each threshold\n",
    "counts_deluxe = [len(fpgrowth(binary_deluxe,  min_support=s, use_colnames=True))\n",
    "                 for s in support_vals]\n",
    "counts_gourmet = [len(fpgrowth(binary_gourmet, min_support=s, use_colnames=True))\n",
    "                  for s in support_vals]\n",
    "\n",
    "# 4. Build a single DataFrame for plotting\n",
    "df_plot = pd.DataFrame({\n",
    "    'Support (%)': list((support_vals*100).round(2))*2,\n",
    "    'NumItemsets': counts_deluxe + counts_gourmet,\n",
    "    'StoreType':    ['Deluxe']*len(support_vals) + ['Gourmet']*len(support_vals)\n",
    "})\n",
    "\n",
    "# 5. Plot side-by-side “facets” with Plotly Express\n",
    "fig = px.line(\n",
    "    df_plot,\n",
    "    x='Support (%)',\n",
    "    y='NumItemsets',\n",
    "    color='StoreType',\n",
    "    facet_col='StoreType',\n",
    "    markers=True,\n",
    "    category_orders={'StoreType': ['Deluxe','Gourmet']},\n",
    "    color_discrete_map={'Deluxe':'orange','Gourmet':'violet'},\n",
    "    title='Elbow Plot: # Frequent Itemsets vs Min Support (FP-Growth)'\n",
    ")\n",
    "\n",
    "# flip X-axis so high support is on left\n",
    "fig.update_xaxes(autorange='reversed')\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\"Elbow point\"*** is very similar between the two subsets and the prior data set. We'll opt to use the same range as prior: 0.5% to 0.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_min_high = 0.003\n",
    "frequent_deluxe = compute_frequent_itemset_subsets(binary_deluxe, S_min_high, \"fp_growth\")\n",
    "print(\"Frequent Deluxe itemsets can be partioned in\",len(frequent_deluxe), \"itemsets of differing lengths.\")\n",
    "\n",
    "S_min_high = 0.003\n",
    "frequent_gourmet = compute_frequent_itemset_subsets(binary_gourmet, S_min_high, \"fp_growth\")\n",
    "print(\"Frequent Gourmet itemsets can be partioned in\",len(frequent_gourmet), \"itemsets of differing lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_min_high = 0.005\n",
    "frequent_deluxe = compute_frequent_itemset_subsets(binary_deluxe, S_min_high, \"fp_growth\")\n",
    "print(\"Frequent Deluxe itemsets can be partioned in\",len(frequent_deluxe), \"itemsets of differing lengths.\")\n",
    "\n",
    "S_min_high = 0.005\n",
    "frequent_gourmet = compute_frequent_itemset_subsets(binary_gourmet, S_min_high, \"fp_growth\")\n",
    "print(\"Frequent Gourmet itemsets can be partioned in\",len(frequent_gourmet), \"itemsets of differing lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 1 items\")\n",
    "print(\"Deluxe\")\n",
    "display(frequent_deluxe[1].sort_values(by='support', ascending=False))\n",
    "print(\"Gourmet\")\n",
    "display(frequent_gourmet[1].sort_values(by='support', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 2 items\")\n",
    "print(\"Deluxe\")\n",
    "display(frequent_deluxe[2].sort_values(by='support', ascending=False))\n",
    "print(\"Gourmet\")\n",
    "display(frequent_gourmet[2].sort_values(by='support', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 3 items\")\n",
    "print(\"Deluxe\")\n",
    "display(frequent_deluxe[3].sort_values(by='support', ascending=False))\n",
    "print(\"Gourmet\")\n",
    "display(frequent_gourmet[3].sort_values(by='support', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both seem pretty similar but already with some minor nuances in the item sets produced. More 3-itemsets in gourmet stores ()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. Generate Association Rules from Frequent Itemsets\n",
    "\n",
    "**This should be trivial now.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Association Rules for Support Threshold and Confidence  - Deluxe vs Gourmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "\n",
    "# thresholds\n",
    "support_levels    = [0.05, 0.01, 0.005, 0.003]\n",
    "confidence_levels = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "def make_confidence_elbow_df(binary_df):\n",
    "    records = []\n",
    "    for sup in support_levels:\n",
    "        freq_itemsets = fpgrowth(binary_df, min_support=sup, use_colnames=True)\n",
    "        # generate all rules at the minimal support but with no confidence filter yet\n",
    "        rules = association_rules(freq_itemsets, metric=\"confidence\", min_threshold=0.0)\n",
    "        for ct in confidence_levels:\n",
    "            records.append({\n",
    "                'Min Support (%)': f\"{sup*100:.1f}\",\n",
    "                'Confidence Threshold': ct,\n",
    "                'Number of Rules': (rules['confidence'] >= ct).sum()\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# prepare data for deluxe and gourmet\n",
    "df_deluxe_conf  = make_confidence_elbow_df(binary_deluxe)\n",
    "df_gourmet_conf = make_confidence_elbow_df(binary_gourmet)\n",
    "\n",
    "# create subplot figure\n",
    "fig = sp.make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Deluxe Supermarkets\",\"Gourmet Supermarkets\"),\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# left: deluxe\n",
    "fig1 = px.line(\n",
    "    df_deluxe_conf,\n",
    "    x=\"Confidence Threshold\", y=\"Number of Rules\",\n",
    "    facet_col=\"Min Support (%)\", facet_col_wrap=2,\n",
    "    markers=True,\n",
    "    color_discrete_sequence=['red']\n",
    ")\n",
    "for trace in fig1.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "# right: gourmet\n",
    "fig2 = px.line(\n",
    "    df_gourmet_conf,\n",
    "    x=\"Confidence Threshold\", y=\"Number of Rules\",\n",
    "    facet_col=\"Min Support (%)\", facet_col_wrap=2,\n",
    "    markers=True,\n",
    "    color_discrete_sequence=['red']\n",
    ")\n",
    "for trace in fig2.data:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "# layout tweaks\n",
    "fig.update_layout(\n",
    "    height=650, width=1200,\n",
    "    title_text=\"Number of Association Rules vs. Confidence Threshold\",\n",
    "    showlegend=False,\n",
    "    margin=dict(t=100, b=80, l=50, r=50)\n",
    ")\n",
    "\n",
    "# axis titles and ticks\n",
    "fig.update_xaxes(title_text=\"Confidence Threshold\", dtick=0.1, row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Confidence Threshold\", dtick=0.1, row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Number of Rules\", row=1, col=1)\n",
    "\n",
    "# simplify facet titles to just the support %\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1] + \"%\"))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Association Rules for Support Threshold and Lift - Deluxe vs Gourmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "# thresholds (add 0.8% support = 0.008)\n",
    "support_levels = [0.05, 0.01, 0.008, 0.005, 0.003]\n",
    "lift_thresholds = np.arange(1.0, 2.1, 0.1)\n",
    "\n",
    "def make_elbow_df(binary_df):\n",
    "    records = []\n",
    "    for sup in support_levels:\n",
    "        freq_itemsets = fpgrowth(binary_df, min_support=sup, use_colnames=True)\n",
    "        rules = association_rules(freq_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "        for lt in lift_thresholds:\n",
    "            records.append({\n",
    "                'Min Support (%)': f\"{sup*100:.1f}%\",\n",
    "                'Lift Threshold': lt,\n",
    "                'Number of Rules': (rules['lift'] >= lt).sum()\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# prepare data for Deluxe & Gourmet\n",
    "df_deluxe_elbow  = make_elbow_df(binary_deluxe)\n",
    "df_gourmet_elbow = make_elbow_df(binary_gourmet)\n",
    "\n",
    "# subplot layout\n",
    "fig = sp.make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Deluxe Supermarkets\",\"Gourmet Supermarkets\"),\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing=0.10\n",
    ")\n",
    "\n",
    "# Deluxe traces\n",
    "fig1 = px.line(\n",
    "    df_deluxe_elbow,\n",
    "    x=\"Lift Threshold\", y=\"Number of Rules\",\n",
    "    facet_col=\"Min Support (%)\", facet_col_wrap=2,\n",
    "    markers=True\n",
    ")\n",
    "for t in fig1.data:\n",
    "    fig.add_trace(t, row=1, col=1)\n",
    "\n",
    "# Gourmet traces\n",
    "fig2 = px.line(\n",
    "    df_gourmet_elbow,\n",
    "    x=\"Lift Threshold\", y=\"Number of Rules\",\n",
    "    facet_col=\"Min Support (%)\", facet_col_wrap=2,\n",
    "    markers=True\n",
    ")\n",
    "for t in fig2.data:\n",
    "    fig.add_trace(t, row=1, col=2)\n",
    "\n",
    "# final layout tweaks\n",
    "fig.update_layout(\n",
    "    height=600, width=1200,\n",
    "    title_text=\"Rules vs. Lift Threshold across Support Levels\",\n",
    "    showlegend=False,\n",
    "    margin=dict(t=80, b=60, l=50, r=50)\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Lift Threshold\", dtick=0.1, row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Lift Threshold\", dtick=0.1, row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Number of Rules\", row=1, col=1)\n",
    "\n",
    "# simplify facet titles\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.replace(\"Min Support (%)=\", \"\")))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in support_levels:\n",
    "    thresholdD = s * binary_deluxe.shape[0]\n",
    "    thresholdG = s * binary_gourmet.shape[0]\n",
    "    print(\n",
    "        f\"At support level {s:.3f} ({s*100:.2f}%): \"\n",
    "        f\"a deluxe itemset must appear in at least {thresholdD:.0f} transactions, and \"\n",
    "        f\"a gourmet itemset must appear in at least {thresholdG:.0f} transactions.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a support threshold of 0.5% or 1% is more appropriate for the Gourmet subset, and for the Deluxe a 0.3% to 1% will still do. (this is kinda arbitrary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deluxe Stores Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_deluxe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_rule_scatter_plotly(\n",
    "        binary_deluxe,\n",
    "        min_support=0.005,\n",
    "        min_confidence=0.10,\n",
    "        min_lift=1.05,\n",
    "        point_size=8,\n",
    "        opacity=0.8\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_rule_scatter_plotly(\n",
    "        binary_deluxe,\n",
    "        min_support=0.005,\n",
    "        min_confidence=0.10,\n",
    "        min_lift=1.20,\n",
    "        point_size=8,\n",
    "        opacity=0.8\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "min_sup   = 0.005\n",
    "min_conf  = 0.12    \n",
    "min_lift  = 1.20\n",
    "\n",
    "\n",
    "freq_items = fpgrowth(\n",
    "    binary_deluxe,\n",
    "    min_support=min_sup,\n",
    "    use_colnames=True\n",
    ")\n",
    "\n",
    "\n",
    "rules = association_rules(\n",
    "    freq_items,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=min_conf\n",
    ")\n",
    "\n",
    "\n",
    "strong_rules = rules[rules['lift'] >= min_lift].copy()\n",
    "\n",
    "\n",
    "strong_rules['count'] = (strong_rules['support'] * binary_deluxe.shape[0]).astype(int)\n",
    "\n",
    "\n",
    "strong_rules = strong_rules.sort_values(['confidence','lift'], ascending=False)\n",
    "\n",
    "print(\n",
    "    f\"Found {len(strong_rules)} rules with support ≥ {min_sup*100:.1f}%, \"\n",
    "    f\"confidence ≥ {min_conf:.2f}, lift ≥ {min_lift:.2f}\\n\"\n",
    ")\n",
    "\n",
    "strong_rules[[\n",
    "    'antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction'\n",
    "]\n",
    "].head(30).sort_values('count', ascending=False).sort_values('lift', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gourmet Store Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_gourmet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_rule_scatter_plotly(\n",
    "        binary_gourmet,\n",
    "        min_support=0.01,\n",
    "        min_confidence=0.10,\n",
    "        min_lift=1.20,\n",
    "        point_size=8,\n",
    "        opacity=0.8\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "min_sup   = 0.01\n",
    "min_conf  = 0.10     \n",
    "min_lift  = 1.20\n",
    "\n",
    "freq_items = fpgrowth(\n",
    "    binary_gourmet,\n",
    "    min_support=min_sup,\n",
    "    use_colnames=True\n",
    ")\n",
    "\n",
    "rules = association_rules(\n",
    "    freq_items,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=min_conf\n",
    ")\n",
    "\n",
    "strong_rules = rules[rules['lift'] >= min_lift].copy()\n",
    "strong_rules['count'] = (strong_rules['support'] * binary_gourmet.shape[0]).astype(int)\n",
    "strong_rules = strong_rules.sort_values(['lift','confidence'], ascending=False)\n",
    "\n",
    "print(\n",
    "    f\"Found {len(strong_rules)} rules with support ≥ {min_sup*100:.1f}%, \"\n",
    "    f\"confidence ≥ {min_conf:.2f}, lift ≥ {min_lift:.2f}\\n\"\n",
    ")\n",
    "\n",
    "strong_rules[[\n",
    "    'antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction'\n",
    "]].head(20).sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.1. Discussing Results\n",
    "\n",
    "##### Deluxe Stores\n",
    "\n",
    "The triple-filter applied to the Deluxe subset (minimum support = 0.30 %, confidence ≥ 0.15 and lift ≥ 1.20) yields twenty-three association rules.  Their supports range from 0.30 % to 0.86 % of the 25 923 Deluxe transactions—equivalent to 78–222 individual baskets—while confidence values span 15 % to almost 46 %.  Lift lies between 1.23 and 1.72, comfortably above statistical independence yet still below the “lock-step” threshold of two that would indicate an almost deterministic link.  As is common with sparse market-basket data, leverage hovers around 0.001 with a standard deviation of roughly 0.0004, and conviction settles in the interval 1.04–1.28: these absolute magnitudes look small because leverage is an unscaled probability difference and conviction is bounded below two when confidence never approaches one.  Nevertheless, both metrics show that the rules reduce prediction error by roughly 100 % relative to chance.\n",
    "\n",
    "Table 1 reproduces the most instructive Deluxe rules—the eight highest-lift links after the filter—and supplements them with a brief merchandising interpretation.\n",
    "\n",
    "| Antecedent ⇒ Consequent                      | Support (%) | Confidence (%) | Lift | Managerial reading                                                                                                                                      |\n",
    "|-----------------------------------------------|-------------|----------------|------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Cookies + Nuts ⇒ Fresh Vegetables             | 0.38        | 44.6           | 1.54 | Shoppers who stock sweet–salty snacks still round out the basket with produce to “balance” the treat.                                                  |\n",
    "| Conditioner ⇒ Fresh Vegetables                | 0.53        | 38.3           | 1.32 | Health-oriented households buy personal-care items and fresh greens in the same trip.                                                                  |\n",
    "| Home Magazines ⇒ Fresh Vegetables             | 0.54        | 36.1           | 1.24 | “Aspirational cooking” effect: décor or lifestyle magazines nudge shoppers toward fresh ingredients.                                                  |\n",
    "| Fresh Fruit + Soup ⇒ Fresh Vegetables         | 0.81        | 35.9           | 1.24 | One-stop meal kit: soup starter, fruit dessert, and vegetables for the main dish.                                                                      |\n",
    "| Cereal + Soup ⇒ Fresh Vegetables              | 0.30        | 35.6           | 1.23 | Families topping up breakfast cereal add vegetables for lunch or dinner on the same visit.                                                            |\n",
    "| Bagels ⇒ Fresh Fruit                          | 0.37        | 25.2           | 1.43 | Classic breakfast pairing: bagels with fruit sides or toppings.                                                                                        |\n",
    "| Paper Wipes + Fresh Vegetables ⇒ Fresh Fruit  | 0.54        | 22.5           | 1.27 | Quick convenience run that mixes household wipes with a full produce refresh.                                                                         |\n",
    "| Soup + Fresh Vegetables ⇒ Fresh Fruit         | 0.81        | 22.3           | 1.27 | Produce-heavy meal planning: vegetables and fruit accompany soup entrées.                                                                              |\n",
    "| Fashion Magazines ⇒ Fresh Fruit               | 0.31        | 22.0           | 1.25 | Lifestyle content prompts healthier food choices, mirroring “magazine diet” behaviour.                                                                 |\n",
    "| Fresh Vegetables + Batteries ⇒ Fresh Fruit    | 0.33        | 21.9           | 1.24 | Small errand (batteries) expands into a produce top-up while already in the store.                                                                    |\n",
    "| Mouthwash ⇒ Fresh Fruit                       | 0.63        | 21.8           | 1.24 | Oral-care purchasers also reach for fresh fruit—consistent with wellness-minded shoppers.                                                              |\n",
    "| Juice + Fresh Vegetables ⇒ Fresh Fruit        | 0.31        | 21.2           | 1.20 | Balanced consumption: fresh juice and produce signal a health-conscious basket.                                                                         |\n",
    "| Oysters ⇒ Fresh Fruit                         | 0.30        | 21.2           | 1.20 | Seafood dinner rounded off with fruit for a light dessert.                                                                                            |\n",
    "| Cereal + Fresh Vegetables ⇒ Soup              | 0.30        | 21.0           | 1.72 | “Two-meal” basket: breakfast items and vegetables today, shelf-stable soup for later.                                                                 |\n",
    "| Nuts + Fresh Vegetables ⇒ Cookies             | 0.38        | 17.8           | 1.69 | Shoppers offset healthier nuts/veggies with a sweet indulgence before checkout.                                                                        |\n",
    "| Hamburger ⇒ Cheese                            | 0.48        | 17.1           | 1.45 | Classic cheeseburger component purchased together.                                                                                                     |\n",
    "| Cereal ⇒ Soup                                 | 0.86        | 15.8           | 1.30 | Comfort-food link: shoppers grabbing cereal for breakfast add soup for quick lunches.                                                                  |\n",
    "| Fresh Fruit + Fresh Vegetables ⇒ Soup         | 0.81        | 15.7           | 1.30 | Produce-heavy baskets still include ready-to-eat soup for convenience.                                                                                 |\n",
    "| Deli Salads ⇒ Cheese                          | 0.65        | 15.7           | 1.34 | Picnic or snack platter: salads and cheese bought together.                                                                                            |\n",
    "| Sour Cream ⇒ Dried Fruit                      | 0.42        | 15.3           | 1.28 | “Sweet-and-savory” snacking: dried fruit paired with dairy-based dips.                                                                                 |\n",
    "| Hard Candy ⇒ Cheese                           | 0.44        | 15.1           | 1.29 | Candy impulse accompanies cheese—possibly for party trays.                                                                                            |\n",
    "| Tuna ⇒ Cheese                                 | 0.44        | 15.1           | 1.29 | Common sandwich fillings purchased side-by-side.                                                                                                       |\n",
    "\n",
    "Ten of the twenty-three Deluxe affinities are symmetric: the reciprocal rule B ⇒ A appears with virtually identical confidence.  Such symmetry indicates co-selection rather than a sequential “trigger” effect.\n",
    "\n",
    "##### Gourmet Stores\n",
    "\n",
    "The Gourmet subset is an order of magnitude smaller (5 328 receipts).  To avoid rules based on fewer than twenty baskets, the support floor was therefore raised to 1 %.  With confidence fixed at 10 % and lift at 1.20, thirteen rules remain.  Their supports cover 1.0–1.8 %, confidence ranges from 10 % to 37 % and lift from 1.22 to 1.42.  Leverage is slightly higher than in Deluxe (0.002–0.004) and conviction approaches 1.10, reflecting the stronger absolute supports.\n",
    "\n",
    "Table 2 lists the seven Gourmet patterns with the greatest explanatory value.\n",
    "\n",
    "| Antecedent ⇒ Consequent | Support (%) | Confidence (%) | Lift | Managerial reading |\n",
    "|-------------------------|-------------|----------------|------|--------------------|\n",
    "| **Pizza** ⇒ Fresh Fruit | 1.37 | **25.0** | **1.42** | Shoppers often offset a ready-to-eat pizza with fresh fruit—dessert for the same meal or lunch-box add-on. |\n",
    "| **Wine** ⇒ Fresh Fruit | 1.84 | 21.9 | 1.24 | Gourmet patrons pair wine with fruit platters or cheese boards; the reverse link (fruit ⇒ wine, 10.4 % confidence) confirms symmetry. |\n",
    "| Canned Vegetables ⇒ Cookies | 1.07 | 14.6 | 1.32 | A pantry restock ends with a sweet impulse treat before checkout. |\n",
    "| **Nasal Sprays** ⇒ Fresh Vegetables | 1.05 | **37.1** | 1.28 | Cold-remedy trips trigger a “healthy-food” complement—customers seek fresh produce for recovery. |\n",
    "| **Mouthwash** ⇒ Fresh Vegetables | 1.03 | 35.5 | 1.23 | Health-conscious baskets that combine oral-care items with nutritious produce. |\n",
    "| **Pot Cleaners** ⇒ Fresh Vegetables | 1.05 | 35.4 | 1.23 | “Clean house, clean diet” behaviour: home-care shoppers add fresh greens in the same visit. |\n",
    "| **Paper Wipes** ⇒ Cookies | 1.09 | 13.7 | 1.23 | Quick convenience run: grab wipes, then reward oneself (or the kids) with biscuits. |\n",
    "| **Jelly** ⇒ Fresh Vegetables | 1.24 | 34.9 | 1.21 | Shoppers picking up preserves also choose fresh produce—perhaps for sandwiches or breakfast spreads. |\n",
    "| **Cheese ⇄ Soup** | 1.78 | 14.7 | 1.22 | Classic comfort-food pairing; the rule appears in both directions, underscoring co-selection rather than causality. |\n",
    "| **Wine ⇄ Soup** | 1.24 | 14.8 / 10.2 | 1.22 | A “gourmet dinner kit” pattern: soup starters paired with wine; symmetry again shows mutual attraction. |\n",
    "\n",
    "\n",
    "Only three Gourmet associations are symmetric, implying that most affinities run one way—from a focal grocery or household staple to a complementary indulgence or healthy offset.\n",
    "\n",
    "Much like the global data set, the small leverage and conviction values deserve a brief methodological note.  Leverage measures the absolute distance between observed and expected co-occurrence.  A value of 0.001 may appear negligible, yet against a baseline expectation of 0.0005 it represents a 100 % improvement.  Conviction only exceeds 1.3 in baskets of four items because true negatives dominate the contingency table. However, conviction greater than one signals that the rule fails less often than independence predicts.\n",
    "\n",
    "Finally, the chosen thresholds are defensible on statistical and managerial grounds.  A 0.30 % support floor in Deluxe (≈ 80 baskets) and 1 % in Gourmet (> 50 baskets) sit well above the frequency expected for two unrelated 4 % items, effectively filtering out random noise.  Confidence requirements of 10–15 % lift the conditional probability to roughly 1.5–2 times the naïve joint rate, while lift ≥ 1.20 lies outside the 95 % independence envelope for datasets of this sparsity.  More stringent cuts (lift > 1.5, leverage ≥ 0.0018, conviction ≥ 1.05) collapse the rule set to a handful of flagship affinities; looser cuts admit hundreds of spurious links.  The adopted triple filter therefore achieves the desired balance: a manageable portfolio of actionable rules that still convey genuine behavioural signals for cross-merchandising and promotional planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.  Take a look at Maximal Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute Maximal Frequent Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute maximal frequent itemsets for Deluxe\n",
    "max_deluxe = fpmax(\n",
    "    binary_deluxe,\n",
    "    min_support=0.005,    # 0.5% threshold\n",
    "    use_colnames=True,\n",
    "    max_len=None\n",
    ")\n",
    "print(f\"Deluxe maximal patterns ({len(max_deluxe)}):\")\n",
    "max_deluxe.sort_values('support', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gourmet maximal patterns\n",
    "max_gourmet = fpmax(\n",
    "    binary_gourmet,\n",
    "    min_support=0.01,    # 1% threshold\n",
    "    use_colnames=True,\n",
    "    max_len=None\n",
    ")\n",
    "print(f\"Deluxe maximal patterns ({len(max_gourmet)}):\")\n",
    "max_gourmet.sort_values('support', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import combinations\n",
    "def mine_maximal_rules_fpmax_full(\n",
    "    binary_df: pd.DataFrame,\n",
    "    *,\n",
    "    min_support: float = 0.005,\n",
    "    min_confidence: float = 0.10,\n",
    "    min_lift: float = 1.1):\n",
    "    \"\"\"\n",
    "    Mine maximal itemsets and derive association rules with full support lookup.\n",
    "\n",
    "    Steps:\n",
    "    1. Mine only the maximal itemsets above `min_support` using FP-Max.\n",
    "       These are the largest frequent sets for which no superset is also frequent.\n",
    "    2. Generate all non-empty proper subsets of each maximal itemset.\n",
    "       This ensures support counts exist for every possible antecedent or consequent.\n",
    "    3. Build a support table by computing, for each subset and each maximal set,\n",
    "       the fraction of transactions containing all items in that set.\n",
    "    4. Call `association_rules` on the combined support table so that every\n",
    "       antecedent/consequent lookup succeeds.\n",
    "    5. Filter the resulting rules by `min_lift`, compute absolute counts,\n",
    "       sort by confidence and lift, and return.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame of association rules with columns:\n",
    "    ['antecedents','consequents','support','confidence','lift','count', …]\n",
    "    \"\"\"\n",
    "    # 1. Mine maximal itemsets\n",
    "    max_sets = fpmax(\n",
    "        binary_df,\n",
    "        min_support=min_support,\n",
    "        use_colnames=True\n",
    "    )\n",
    "\n",
    "    # 2. Gather all non‐empty proper subsets of each maximal set\n",
    "    all_subsets = set()\n",
    "    for s in max_sets['itemsets']:\n",
    "        for k in range(1, len(s)):\n",
    "            for comb in combinations(s, k):\n",
    "                all_subsets.add(frozenset(comb))\n",
    "\n",
    "    # 3. Build a DataFrame of support for every subset + each maximal set\n",
    "    n_tx = binary_df.shape[0]\n",
    "    support_records = []\n",
    "    for itemset in all_subsets.union(max_sets['itemsets']):\n",
    "        mask = binary_df[list(itemset)].all(axis=1)\n",
    "        support_records.append({\n",
    "            'itemsets': itemset,\n",
    "            'support': mask.sum() / n_tx\n",
    "        })\n",
    "    support_table = pd.DataFrame(support_records)\n",
    "\n",
    "    # 4. Generate rules\n",
    "    rules = association_rules(\n",
    "        support_table,\n",
    "        metric=\"confidence\",\n",
    "        min_threshold=min_confidence\n",
    "    )\n",
    "\n",
    "    # 5. Filter by lift, add counts, sort\n",
    "    rules = rules[rules['lift'] >= min_lift].copy()\n",
    "    rules['count'] = (rules['support'] * n_tx).round().astype(int)\n",
    "    return rules.sort_values(['confidence','lift'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Usage\n",
    "rules_deluxe  = mine_maximal_rules_fpmax_full(binary_deluxe,  min_support=0.005, min_confidence=0.15, min_lift=1.15)\n",
    "rules_gourmet = mine_maximal_rules_fpmax_full(binary_gourmet, min_support=0.01, min_confidence=0.10, min_lift=1.20)\n",
    "\n",
    "\n",
    "print(f\"Deluxe Maximal Rules:\")\n",
    "display(rules_deluxe[['antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction']\n",
    "])\n",
    "\n",
    "print(f\"Gourmet Maximal Rules:\")\n",
    "display(rules_gourmet[['antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction']\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.  Deluxe and Gourmet Supermarkets versus each other and versus All Stores (Global versus Deluxe/Gourmet Supermarkets Specific Patterns and Rules)\n",
    "\n",
    "Discuss the similarities and differences between the results obtained in task 1. (frequent itemsets and association rules found in transactions from all stores) and those obtained above (frequent itemsets and association rules found in transactions only from Deluxe and Gourmet Supermarkets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.1 Task 1 Rules vs Task 2 Rules\n",
    "\n",
    "Shoppers across the entire chain reveal a handful of broad, classic grocery affinities—Conditioner → Fresh Vegetables, Rice → Fresh Fruit, TV Dinner → Fresh Fruit—when we set a moderate bar (sup ≥ 0.3%, conf ≥ 20%, lift ≥ 1.20). These three rules capture everyday “stock-up” behaviors that cut across formats: a bit of personal-care sneaks into produce, grains lead to fruit, and ready-meals accompany fresh snacks. When we tighten our lens to only the strongest lifts (≥ 1.30), we unearth a richer tapestry — Hot Dogs ↔ Pasta, Deli Salads ↔ Juice, Pizza ↔ Ice Cream and even Cleaners ↔ Chocolate Candy — yet the playbook remains familiar: staples, treats, and the occasional home-care impulse.\n",
    "\n",
    "Stepping into the Deluxe stores, the narrative shifts. At sup ≥ 0.5%, conf ≥ 12%, lift ≥ 1.20 we still see Deli Salads → Cheese and Juice → Soup, but now they share billing with Conditioner → Fresh Vegetables, Mouthwash → Fresh Fruit, Home Magazines → Fresh Vegetables and Paper Wipes → Fresh Fruit. It’s as though the Deluxe shopper treats each basket as both a self-care ritual and a gourmet snack run—mixing shampoo with salad greens, magazines with vegetables, and personal hygiene with produce in ways the global analysis only hinted at. This could be explain by the Deluxe stores design and item-positioning.\n",
    "\n",
    "In the Gourmet boutiques (sup ≥ 1.0%, conf ≥ 10%, lift ≥ 1.20), by contrast, non-food items drop away entirely. The story becomes pure culinary choreography: Fresh Fruit ↔ Wine, Cheese ↔ Soup, Pizza → Fresh Fruit, Jelly → Fresh Vegetables. Here the basket is a chef’s sketchpad—artisanal preserves, premium produce, curated pantry staples—untarnished by toothpaste or cleaners.\n",
    "\n",
    "Bottom line: the all-stores rules sketch the franchise-wide backbone of grocery behavior—produce paired with meals, occasional personal-care cross-overs. Deluxe builds on that backbone by weaving in lifestyle and home-care purchases, yielding hybrid “self-care + salad” rules. Gourmet, in turn, strips away the non-food notes and amplifies the core food-to-food harmonies, revealing a laser-focused, premium-only shopping mission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.2 Deluxe vs Gourmet Supermarkets\n",
    "\n",
    "Before diving into numbers, it helps to step back and ask: do Deluxe and Gourmet shoppers behave fundamentally like each other, or does each format attract a subtly different mission? In both rule-sets you’ll see familiar grocery pairings — produce with prepared foods, soup with wine—but the nuances reveal how these two store types shape buying patterns.\n",
    "\n",
    "In the Deluxe rules (sup ≥ 0.5%, conf ≥ 0.12, lift ≥ 1.20), we uncover a mix of food to food and non-food to food connections. Deli Salads → Cheese tops the list (lift ≈ 1.34), underscoring a deli-centric mission: shoppers grabbing ready-to-eat items then adding complementary cheeses. Equally notable are cross-category “impulse” links—Conditioner → Fresh Vegetables and Mouthwash → Fresh Fruit—suggesting that Deluxe customers often combine personal-care or home-care purchases with fresh produce in a single visit: it could signify that consumers thinking of personal care also think of their diet as well, or perhaps the positioning of the Deluxe supermarkets invites the shoppers' eye to the Fresh foods part after visiting the personal care section. The breadth of antecedents (Paper Wipes, Home Magazines, Personal Hygiene) hints at a one-stop premium experience.\n",
    "\n",
    "By contrast, the Gourmet rules (sup ≥ 1.0%, conf ≥ 0.10, lift ≥ 1.20) refocus on classic food-only affinities, as expected for this type of food-dedicated type of stores. Fresh Fruit ↔ Wine and Cheese ↔ Soup appear immediately, reflecting the refined, food-centric mission of a high-end gourmet shop. The strongest lift (Pizza → Fresh Fruit, lift ≈ 1.42) still ties produce to prepared foods, but personal-care items vanish—gourmet customers rarely mix non-food in these transactions. Instead, they concentrate on artisanal staples: Jelly → Fresh Vegetables and Canned Vegetables → Cookies point to curated pantry builds.\n",
    "\n",
    "In other words, both formats share the chain-wide backbone—produce pairs with prepared foods, soup with wine—but Deluxe extends that backbone into adjacent categories (home and personal care), while Gourmet doubles down on pure food craftsmanship. Deluxe rules reveal a hybrid grocery+“lifestyle” basket; Gourmet rules reveal a laser-focus on premium edibles. This suggests that store designs could be different and catch the eye of the client, producing these different item-type combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Analyse Small Groceries\n",
    "\n",
    "Here you should analyse **Small Groceries (STORE_ID = 2, 5, 14, 22)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.  Load/Preprocess the Dataset\n",
    "\n",
    "**This should be trivial now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Foodmart_2025_DM.csv\"\n",
    "df_all = load_transactions(filename)\n",
    "\n",
    "# Drop transactions with no STORE_ID\n",
    "df_all = df_all.dropna(subset=['STORE_ID']).copy()\n",
    "\n",
    "# (Optional) if STORE_ID was float after dropping, cast to int\n",
    "df_all['STORE_ID'] = df_all['STORE_ID'].astype(int)\n",
    "\n",
    "small_ids = [2, 5, 14, 22]\n",
    "\n",
    "small_df = df_all[df_all['STORE_ID'].isin(small_ids)]\n",
    "\n",
    "print(f\"Small Groceries Supermarkets: {len(small_df)} transactions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Into Binary Sparse Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_small  = small_df['items'].tolist()\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te.fit(df_all['items'].tolist())\n",
    "\n",
    "# transform each subset\n",
    "te_ary_small  = te.transform(transactions_small)\n",
    "\n",
    "# binary\n",
    "binary_small  = pd.DataFrame(te_ary_small,  columns=te.columns_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_stats(name, df):\n",
    "    n_trans = len(df)\n",
    "    n_columns = df.shape[1]\n",
    "    basket_sizes = df.sum(axis=1)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Transactions       = {n_trans:,}\")\n",
    "    print(f\"  Columns            = {n_columns}\")\n",
    "    print(f\"  Avg items/basket   = {basket_sizes.mean():.2f}\")\n",
    "    print(f\"  Median items/basket= {basket_sizes.median():.0f}\")\n",
    "    print()\n",
    "\n",
    "dataset_stats(\"Small Groceries Supermarkets\", binary_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_small.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute relative frequencies for the small grocery dataset\n",
    "rel_small = (binary_small.sum(axis=0) / len(binary_small) * 100).sort_values(ascending=False)\n",
    "\n",
    "df_counts = pd.DataFrame({\n",
    "    'Item': rel_small.index,\n",
    "    'Relative Frequency (%)': rel_small.values\n",
    "})\n",
    "\n",
    "# Plot the relative frequencies\n",
    "top_items = df_counts.head(30)  # top 30 items only\n",
    "fig = px.bar(\n",
    "    top_items,\n",
    "    x='Item',\n",
    "    y='Relative Frequency (%)',\n",
    "    title='Top 30 Items by Transaction Frequency',\n",
    "    labels={'Item': 'Product', 'Relative Frequency (%)': 'Percentage of Transactions'}\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=60, b=150)\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Item Relative Frequencies / Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have the ***\"long tail\"*** distribution in these 2 subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Compute Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This should be trivial now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing Elbow points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vals = np.arange(0.05, 0.0009, -0.001)\n",
    "\n",
    "# frequent itemsets at each threshold for small groceries\n",
    "counts_small = [len(fpgrowth(binary_small, min_support=s, use_colnames=True))\n",
    "                for s in support_vals]\n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    'Support (%)': (support_vals*100).round(2),\n",
    "    'NumItemsets': counts_small\n",
    "})\n",
    "\n",
    "fig = px.line(\n",
    "    df_plot,\n",
    "    x='Support (%)',\n",
    "    y='NumItemsets',\n",
    "    markers=True,\n",
    "    title='Frequent Itemsets vs Min Support (FP-Growth) for Small Groceries',\n",
    "    labels={'Support (%)': 'Minimum Support (%)', 'NumItemsets': 'Number of Frequent Itemsets'}\n",
    ")\n",
    "\n",
    "fig.update_xaxes()\n",
    "fig.update_layout(height=500, width=800, showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\"Elbow point\"*** is very similar between the two subsets and the prior data set. We'll opt to use the same range as prior: 0.5% to 0.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_min_high = 0.003\n",
    "frequent_small = compute_frequent_itemset_subsets(binary_small, S_min_high, \"fp_growth\")\n",
    "print(\"Frequent Small Groceries itemsets can be partioned in\",len(frequent_small), \"itemsets of differing lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 1 item\")\n",
    "print(\"Small Groceries\")\n",
    "display(frequent_small[1].sort_values(by='support', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 2 items\")\n",
    "print(\"Small Groceries\")\n",
    "display(frequent_small[2].sort_values(by='support', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 3 items\")\n",
    "print(\"Small Groceries\")\n",
    "display(frequent_small[3].sort_values(by='support', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems 0.003 min_supp is capturing a large number of samples which can bring noise and a higher computational effort. Let's raise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_min_high = 0.004\n",
    "frequent_small = compute_frequent_itemset_subsets(binary_small, S_min_high, \"fp_growth\")\n",
    "print(\"Frequent Small Groceries itemsets can be partioned in\",len(frequent_small), \"itemsets of differing lengths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 1 item\")\n",
    "print(\"Small Groceries\")\n",
    "display(frequent_small[1].sort_values(by='support', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 2 items\")\n",
    "print(\"Small Groceries\")\n",
    "display(frequent_small[2].sort_values(by='support', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequent itemsets of 3 items\")\n",
    "print(\"Small Groceries\")\n",
    "display(frequent_small[3].sort_values(by='support', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are better to work on and should still catch good assocation rules!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Generate Association Rules from Frequent Itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "support_levels    = [0.05, 0.01, 0.0075, 0.004]\n",
    "confidence_levels = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "def make_confidence_elbow_df(binary_df):\n",
    "    records = []\n",
    "    for sup in support_levels:\n",
    "        # get all itemsets at this support\n",
    "        freq_itemsets = fpgrowth(binary_df, min_support=sup, use_colnames=True)\n",
    "        # generate rules with no confidence threshold\n",
    "        rules = association_rules(freq_itemsets, metric=\"confidence\", min_threshold=0.0)\n",
    "        for ct in confidence_levels:\n",
    "            records.append({\n",
    "                'Min Support (%)': f\"{sup*100:.1f}\",\n",
    "                'Confidence Threshold': ct,\n",
    "                'Number of Rules': int((rules['confidence'] >= ct).sum())\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# build the elbow DataFrame for your binary_small set\n",
    "df_small_conf = make_confidence_elbow_df(binary_small)\n",
    "\n",
    "# plot\n",
    "fig = px.line(\n",
    "    df_small_conf,\n",
    "    x=\"Confidence Threshold\",\n",
    "    y=\"Number of Rules\",\n",
    "    facet_col=\"Min Support (%)\",\n",
    "    facet_col_wrap=2,\n",
    "    markers=True,\n",
    "    color_discrete_sequence=['red'],\n",
    "    title=\"Association‐Rule Counts vs Confidence Threshold (binary_small)\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1000, width=1200,\n",
    "    showlegend=False,\n",
    "    margin=dict(t=100, b=80, l=50, r=50)\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Confidence Threshold\", dtick=0.1)\n",
    "fig.update_yaxes(title_text=\"Number of Rules\")\n",
    "\n",
    "# simplify facet titles to just \"5.0%\", \"1.0%\", etc.\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1] + \"%\"))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in support_levels:\n",
    "    threshold = s * binary_small.shape[0]\n",
    "    print(\n",
    "        f\"At support level {s:.3f} ({s*100:.2f}%), \"\n",
    "        f\"an itemset must appear in at least {threshold:.0f} transactions.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_rule_scatter_plotly(\n",
    "        binary_small,\n",
    "        min_support=0.0075,\n",
    "        min_confidence=0.15,\n",
    "        min_lift=1.2,\n",
    "        point_size=8,\n",
    "        opacity=0.8\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "min_sup   = 0.0075\n",
    "min_conf  = 0.15     \n",
    "min_lift  = 1.20\n",
    "\n",
    "freq_items = fpgrowth(\n",
    "    binary_small,\n",
    "    min_support=min_sup,\n",
    "    use_colnames=True\n",
    ")\n",
    "\n",
    "rules = association_rules(\n",
    "    freq_items,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=min_conf\n",
    ")\n",
    "\n",
    "strong_rules_075 = rules[rules['lift'] >= min_lift].copy()\n",
    "strong_rules_075['count'] = (strong_rules_075['support'] * binary_small.shape[0]).astype(int)\n",
    "strong_rules_075 = strong_rules_075.sort_values(['lift','confidence'], ascending=False)\n",
    "\n",
    "print(\n",
    "    f\"Found {len(strong_rules_075)} rules with support ≥ {min_sup*100:.1f}%, \"\n",
    "    f\"confidence ≥ {min_conf:.2f}, lift ≥ {min_lift:.2f}\\n\"\n",
    ")\n",
    "\n",
    "strong_rules_075[['antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction']].head(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_rule_scatter_plotly(\n",
    "        binary_small,\n",
    "        min_support=0.015,\n",
    "        min_confidence=0.15,\n",
    "        min_lift=1.2,\n",
    "        point_size=8,\n",
    "        opacity=0.8\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "min_sup   = 0.015\n",
    "min_conf  = 0.15     \n",
    "min_lift  = 1.20\n",
    "\n",
    "freq_items = fpgrowth(\n",
    "    binary_small,\n",
    "    min_support=min_sup,\n",
    "    use_colnames=True\n",
    ")\n",
    "\n",
    "rules = association_rules(\n",
    "    freq_items,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=min_conf\n",
    ")\n",
    "\n",
    "strong_rules_15 = rules[rules['lift'] >= min_lift].copy()\n",
    "strong_rules_15['count'] = (strong_rules_15['support'] * binary_small.shape[0]).astype(int)\n",
    "strong_rules_15 = strong_rules_15.sort_values(['lift','confidence'], ascending=False)\n",
    "\n",
    "print(\n",
    "    f\"Found {len(strong_rules_15)} rules with support ≥ {min_sup*100:.1f}%, \"\n",
    "    f\"confidence ≥ {min_conf:.2f}, lift ≥ {min_lift:.2f}\\n\"\n",
    ")\n",
    "\n",
    "strong_rules_15[['antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write text in cells like this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the relatively high 'head' of our distribution of frequencies, let's do as previously and check the rules **without Fresh Fruits and Fresh Vegetables**, the top 2 frequency items above the elbow point of the item frequency x name of item graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3.1 Association Rules Filtering the High Frequency Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_items = {'Fresh Fruit', 'Fresh Vegetables'}\n",
    "\n",
    "# For support ≥ 0.008 (0.8%)\n",
    "filtered_rules_08 = strong_rules_08[\n",
    "    ~strong_rules_08['antecedents'].apply(lambda x: any(item in x for item in exclude_items)) &\n",
    "    ~strong_rules_08['consequents'].apply(lambda x: any(item in x for item in exclude_items))\n",
    "]\n",
    "\n",
    "print(f\"After filtering, {len(filtered_rules_08)} rules remain (from original 21 at 0.8% support).\")\n",
    "\n",
    "# For support ≥ 0.015 (1.5%)\n",
    "filtered_rules_15 = strong_rules_15[\n",
    "    ~strong_rules_15['antecedents'].apply(lambda x: any(item in x for item in exclude_items)) &\n",
    "    ~strong_rules_15['consequents'].apply(lambda x: any(item in x for item in exclude_items))\n",
    "]\n",
    "\n",
    "print(f\"After filtering, {len(filtered_rules_15)} rules remain (from original 7 at 1.5% support).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rules_08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rules_15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial analysis of association rules, \"Fresh Fruit\" and \"Fresh Vegetables\" frequently appeared as antecedents or consequents due to their high frequency in the dataset. While their frequent co-occurrence with other products might seem insightful at first glance, there are a few reasons to critically assess and filter these items when our goal is to uncover more nuanced or hidden patterns in the data: their overrepresentation can obscure more meaningful patterns and lead to rules that are not actionable, as well as inflate the confidence and lift.\n",
    "\n",
    "Once we removed rules containing \"Fresh Fruit\" and \"Fresh Vegetables\", we observed a notable drop in the number of strong rules: at 0.8% support, rules dropped from 21 → 8; at 1.5% support, rules dropped from 7 → 2.\n",
    "\n",
    "This reduction confirms how influential these items are — but also emphasizes that a smaller set of more meaningful rules remain when we control for their dominance.\n",
    "\n",
    "The filtered rules now reflect more specific and targeted relationships such as: \n",
    "(Dried Fruit) ↔ (Cheese), \n",
    "(Jam) → (Cookies), \n",
    "(Juice) → (Dried Fruit)\n",
    "\n",
    "These may reveal complementary purchases or niche customer behaviors (e.g., snack pairings, dessert planning, or pantry restocks) that can be more valuable for marketing campaigns, cross-promotions, or shelf adjacency planning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Take a Look at Maximal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* ``binary_small`` must be a pandas DataFrame whose columns are item names and\n",
    "  whose rows are transactions, with 1/0 (or True/False) entries marking item\n",
    "  presence.\n",
    "* Adjust ``min_support`` and ``min_conf`` as appropriate for your dataset size.\n",
    "* The script first mines **all** frequent itemsets via Apriori, then filters\n",
    "  them down to the **maximal** ones (no proper superset is also frequent).\n",
    "* Finally it generates standard association rules and shows how to keep only\n",
    "  those whose full itemset (antecedent ∪ consequent) is one of the maximal\n",
    "  patterns if desired.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Load or define your dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "# binary_small = pd.read_csv(\"binary_small.csv\")\n",
    "# OR if you're in a notebook it may already be in memory:\n",
    "# from your_preprocessing_module import binary_small\n",
    "\n",
    "# For demonstration here we raise an error if the name is missing so the user\n",
    "# is reminded to supply the DataFrame.\n",
    "try:\n",
    "    binary_small  # noqa: F821  # type: ignore\n",
    "except NameError as e:\n",
    "    raise NameError(\n",
    "        \"`binary_small` DataFrame is not defined. Load or create it before running.\"  # noqa: E501\n",
    "    ) from e\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Mine **all** frequent itemsets with Apriori\n",
    "# -----------------------------------------------------------------------------\n",
    "min_support = 0.01        # pick a sensible threshold for your data size\n",
    "frequent_itemsets = apriori(\n",
    "    binary_small,\n",
    "    min_support=min_support,\n",
    "    use_colnames=True,\n",
    "    low_memory=True,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Extract **maximal** frequent itemsets\n",
    "# -----------------------------------------------------------------------------\n",
    "# A frequent itemset is maximal if no proper superset of it is also frequent.\n",
    "\n",
    "def is_subset_of_any(candidate, itemsets):\n",
    "    \"\"\"Return True if *candidate* is a strict subset of *any* set in *itemsets*.\"\"\"\n",
    "    return any((candidate < other) for other in itemsets if candidate != other)\n",
    "\n",
    "maximal_mask = ~frequent_itemsets[\"itemsets\"].apply(\n",
    "    lambda x: is_subset_of_any(x, frequent_itemsets[\"itemsets\"].values)\n",
    ")\n",
    "maximal_itemsets = frequent_itemsets.loc[maximal_mask].reset_index(drop=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Generate association rules\n",
    "# -----------------------------------------------------------------------------\n",
    "min_conf = 0.10            # confidence threshold\n",
    "min_lift = 1.20           # lift threshold\n",
    "rules = association_rules(\n",
    "    frequent_itemsets,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=min_conf,\n",
    ")\n",
    "strong_rules = rules[rules['lift'] >= min_lift].copy()\n",
    "strong_rules['count'] = (strong_rules['support'] * binary_small.shape[0]).astype(int)\n",
    "strong_rules = strong_rules.sort_values(['lift','confidence'], ascending=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. (Option A) Use **all** rules\n",
    "# -----------------------------------------------------------------------------\n",
    "all_rules = strong_rules.sort_values(\"lift\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. (Option B) Keep only rules whose full itemset is maximal\n",
    "# -----------------------------------------------------------------------------\n",
    "maximal_sets = set(maximal_itemsets[\"itemsets\"].apply(frozenset))\n",
    "rules_maximal = rules[\n",
    "    rules.apply(lambda r: (r[\"antecedents\"] | r[\"consequents\"]) in maximal_sets, axis=1)\n",
    "].sort_values(\"lift\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. Display or save results\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n=== Maximal frequent itemsets ===\")\n",
    "display(maximal_itemsets.sort_values(\"support\", ascending=False))\n",
    "\n",
    "print(\"\\n=== Association rules (all) ===\")\n",
    "display(all_rules[['antecedents', 'consequents', 'antecedent support', 'consequent support',\n",
    "    'support', 'count',\n",
    "    'confidence', 'lift',\n",
    "    'leverage', 'conviction']])\n",
    "\n",
    "print(\"\\n=== Association rules where antecedent u consequent is maximal ===\")\n",
    "display(rules_maximal[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write text in cells like this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5. Small Groceries versus All Stores (Global versus Small Groceries Specific Patterns and Rules)\n",
    "\n",
    "Discuss the similarities and diferences between the results obtained in task 1. (frequent itemsets and association rules found in transactions from all stores) and those obtained above (frequent itemsets and association rules found in transactions only Small Groceries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Rule Volume & Thresholds**  \n",
    "   - **All stores** (Task 1) yielded **22 strong rules** at `min_sup = 0.3%`, `min_conf = 6%`, `min_lift = 1.3`.  \n",
    "   - **Small groceries** (Task 2) produced **21 rules** at `min_sup = 0.8%`, `min_conf = 15%`, `min_lift = 1.2`, and only **7 rules** at a higher `min_sup = 1.5%`.  \n",
    "   This reflects both the reduced transaction volume in Small Groceries and the need for higher support/confidence to isolate robust patterns.\n",
    "\n",
    "\n",
    "2. **Category Focus**  \n",
    "   - **All stores** rules feature a **wide variety** of cross-category pairings (e.g., *Hot Dogs ↔ Pasta, Deli Salads ↔ Juice, Frozen Chicken ↔ Waffles*), highlighting general purchasing trends across diverse product lines.  \n",
    "   - **Small groceries** rules skew heavily toward **fresh produce** and **household essentials** (e.g., *Fresh Fruit/Vegetables → Paper Wipes, Dried Fruit ↔ Cheese, Jam → Cookies*), reflecting the narrower assortment typical of small grocery outlets.\n",
    "\n",
    "3. **Strength of Associations**  \n",
    "   - Top lifts in **all stores** exceed **1.50** (e.g., *Hot Dogs → Pasta, Lift≈1.52*), but many rules cluster just above **1.30**.  \n",
    "   - In **Small Groceries**, the strongest link is *Fresh Fruit & Fresh Vegetables → Paper Wipes* with **Lift≈2.17**, indicating a very pronounced co-purchase trend in that segment. Even the lower-lift patterns still exceed **1.20**, showing consistently meaningful associations despite higher support/confidence requirements.\n",
    "\n",
    "4. **Symmetry vs. Directionality**  \n",
    "   - Many **global rules** appear in **both directions** (e.g., *Pasta → Hot Dogs* and *Hot Dogs → Pasta*), owing to similar support levels in a large dataset.  \n",
    "   - **Small Groceries** rules tend to be **directional**—for example, *Dried Fruit → Cheese* remains significant, whereas the reverse holds, but only at the lower support threshold.\n",
    "\n",
    "5. **Practical Implications**  \n",
    "   - **Global patterns** can inform broad merchandising and cross-category promotions at scale.  \n",
    "   - **Small-groceries patterns** suggest targeted bundling opportunities (e.g., placing paper wipes near fresh produce) and niche promotions (e.g., cheese with dried fruit) that maximize the limited shelf space of these smaller stores.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write text in cells like this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.  Deluxe and Gourmet Supermarkets versus Small Groceries\n",
    "\n",
    "Discuss the similarities and diferences between the results obtained in task 2.1. (frequent itemsets and association rules found in transactions only from Deluxe/Gourmet Supermarkets) and those obtained in task 2.2. (frequent itemsets and association rules found in transactions only Small Groceries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tasks 2.1 and 2.2 we independently mined association rules—first on transactions from Deluxe/Gourmet supermarkets (support ≥ 1.0 %, confidence ≥ 10 %, lift ≥ 1.20), then on Small-Groceries transactions (support ≥ 0.8 % and 1.5 %, confidence ≥ 15 %, lift ≥ 1.20). Here are the key similarities and contrasts:\n",
    "\n",
    "1. **Overall Rule Counts & Threshold Effects**  \n",
    "   - Both segments yielded roughly a dozen strong rules when moderate support and confidence thresholds were applied.  \n",
    "   - Tightening support (from 0.8 % to 1.5 % in Small-Groceries) pruned many lower-frequency patterns, leaving only the strongest two (Dried Fruit ⇄ Cheese).\n",
    "\n",
    "2. **Dominant Item Categories**  \n",
    "   - **Fresh produce** (Fruit & Vegetables) appears in the antecedent or consequent in most rules, across both store types.  \n",
    "   - **Packaged staples** (Soup, Cheese, Cookies, Dried Fruit) also recur—pointing to common cross-selling opportunities everywhere.\n",
    "\n",
    "3. **Lift & Confidence Patterns**  \n",
    "   - **Deluxe/Gourmet rules** tend to have moderate lifts (1.20–1.42) but fairly wide confidence ranges (10–37 %), reflecting more varied, less-frequent purchase combinations.  \n",
    "   - **Small-Groceries rules** show higher lifts (up to 2.17) and higher confidences (15–37 %), indicating tighter, more predictable buying patterns in a smaller-SKU environment.\n",
    "\n",
    "4. **Unique Cross-Sells**  \n",
    "   - **Deluxe/Gourmet**: Specialty pairs like Fresh Fruit → Wine, Soup ⇄ Cheese, Jelly → Fresh Vegetables.  \n",
    "   - **Small-Groceries**: Everyday combos such as Juice → Dried Fruit, Jam → Cookies, Paper Wipes → Fresh Fruit. This suggests distinct promotional bundling strategies.\n",
    "\n",
    "5. **Implications for Merchandising**  \n",
    "   - In Gourmet channels, highlight premium cross-sells (e.g. fruit + wine displays).  \n",
    "   - In Small-Groceries, focus on convenience pairings (e.g. place dried fruits near juice, jam near cookies).\n",
    "\n",
    "By comparing these two segments side-by-side, we can tailor marketing and shelf layouts to the distinct shopping behaviors in each store type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
